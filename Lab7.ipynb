{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbJRsoxUGSlAqSRmpC1RII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-spangher/EA_LSTM/blob/master/Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ_q2PkVeL2R",
        "outputId": "ea6589f6-061e-43af-9caa-b3bdb8902a29"
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ‚ùØ \u001b[0mlucas-spangher\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0ms\u001b[0mo\u001b[0mc\u001b[0mi\u001b[0ma\u001b[0ml\u001b[0m-\u001b[0mg\u001b[0ma\u001b[0mm\u001b[0me\u001b[0m-\u001b[0mr\u001b[0ml\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0;38;5;214;1m lucas-spangher\u001b[43D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ‚ùØ \u001b[0mfsdl_lab_5\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0mC\u001b[0mr\u001b[0me\u001b[0ma\u001b[0mt\u001b[0me\u001b[0m \u001b[0mN\u001b[0me\u001b[0mw\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0;38;5;214;1m fsdl_lab_5\u001b[42D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"fsdl_lab_5\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlOKhUQ1PpQo",
        "outputId": "bd50e96d-0a99-499f-edb0-1a91390e8d7d"
      },
      "source": [
        "# FSDL Spring 2021 Setup\n",
        "!git clone https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n",
        "%cd fsdl-text-recognizer-2021-labs\n",
        "!pip install boltons pytorch_lightning==1.1.4 wandb\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fsdl-text-recognizer-2021-labs'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 354 (delta 158), reused 296 (delta 109), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 3.83 MiB | 32.70 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n",
            "/content/fsdl-text-recognizer-2021-labs\n",
            "Collecting boltons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e1/e7979a4a6d4b296b5935e926549fff540f7670ddaf09bbf137e2b022c039/boltons-20.2.1-py2.py3-none-any.whl (170kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 174kB 17.7MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning==1.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 686kB 43.2MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.0MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (4.41.1)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645kB 48.1MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.19.5)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112kB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102kB 13.2MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 53.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->pytorch_lightning==1.1.4) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.27.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (3.7.0)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (3.4.1)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (20.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296kB 53.0MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143kB 51.9MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.1.0)\n",
            "Building wheels for collected packages: future, subprocess32, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=f23caeee03130dfec60417a30ddccf3a83b788407df182ab2ec0fa6e8f35a12c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=a7473550cbde761e78dff428d4d610d20ea5084943101bd7eb96c0020c479544\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=f0ceb5e6288b7f97f68705d740d78ff74cfc4e473a8c0ef12eb19f96f91f3eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built future subprocess32 pathtools\n",
            "Installing collected packages: boltons, PyYAML, future, async-timeout, multidict, yarl, aiohttp, fsspec, pytorch-lightning, subprocess32, configparser, sentry-sdk, docker-pycreds, pathtools, shortuuid, smmap, gitdb, GitPython, wandb\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.14 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 boltons-20.2.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-0.8.7 future-0.18.2 gitdb-4.0.5 multidict-5.1.0 pathtools-0.1.2 pytorch-lightning-1.1.4 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21 yarl-1.6.3\n",
            "env: PYTHONPATH=.:$PYTHONPATH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcP36XkzT0OY",
        "outputId": "42920d54-22f6-48a1-bcd1-a3bce89cb720"
      },
      "source": [
        "cd lab5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwvITJRUaGa4"
      },
      "source": [
        "Set up wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo5n3pTtTmoq",
        "outputId": "ea6589f6-061e-43af-9caa-b3bdb8902a29"
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ‚ùØ \u001b[0mlucas-spangher\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0ms\u001b[0mo\u001b[0mc\u001b[0mi\u001b[0ma\u001b[0ml\u001b[0m-\u001b[0mg\u001b[0ma\u001b[0mm\u001b[0me\u001b[0m-\u001b[0mr\u001b[0ml\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0;38;5;214;1m lucas-spangher\u001b[43D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ‚ùØ \u001b[0mfsdl_lab_5\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0mC\u001b[0mr\u001b[0me\u001b[0ma\u001b[0mt\u001b[0me\u001b[0m \u001b[0mN\u001b[0me\u001b[0mw\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0;38;5;214;1m fsdl_lab_5\u001b[42D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"fsdl_lab_5\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyMaDanna-Bf",
        "outputId": "31f472bf-1790-42e0-d431-bfab24d2e552"
      },
      "source": [
        "!python training/run_experiment.py --wandb --max_epochs=10 --gpus='0,' --num_workers=4 --data_class=EMNISTLines2 --model_class=LineCNNTransformer --loss=transformer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 00:10:07.722666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-glitter-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/1lbo72mk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "EMNISTLines2 generating data for train...\n",
            "[nltk_data] Downloading package brown to /content/fsdl-text-\n",
            "[nltk_data]     recognizer-2021-labs/data/downloaded/nltk...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "Downloading raw dataset from https://s3-us-west-2.amazonaws.com/fsdl-public-assets/matlab.zip to /content/fsdl-text-recognizer-2021-labs/data/downloaded/emnist/matlab.zip...\n",
            "709MB [00:21, 34.8MB/s]                           \n",
            "Computing SHA-256...\n",
            "Unzipping EMNIST...\n",
            "Loading training data from .mat file\n",
            "Balancing classes to reduce amount of data\n",
            "Saving to HDF5 in a compressed format...\n",
            "Saving essential dataset parameters to text_recognizer/datasets...\n",
            "Cleaning up...\n",
            "tcmalloc: large alloc 2293760000 bytes == 0x55b036866000 @  0x7f3c99674b6b 0x7f3c99694379 0x7f3c382ab25e 0x7f3c382ac9d2 0x7f3c74f6bad6 0x7f3c753cdff9 0x7f3c758d839a 0x7f3c758a3b19 0x7f3c7585a277 0x7f3c756fe549 0x7f3c753d1b2c 0x7f3c759dee29 0x7f3c759df0aa 0x7f3c758b2e59 0x7f3c75866f3e 0x7f3c7573a11b 0x7f3c86aef783 0x55afe1b40050 0x55afe1b3fde0 0x55afe1bb4244 0x55afe1b4169a 0x55afe1bafa45 0x55afe1b4169a 0x55afe1bafc9e 0x55afe1a80d14 0x55afe1bb11e6 0x55afe1baee0d 0x55afe1a80eb0 0x55afe1bb11e6 0x55afe1baee0d 0x55afe1b4177a\n",
            "EMNISTLines2 generating data for val...\n",
            "EMNISTLines2 generating data for test...\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 4.3 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.6 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "4.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.3 M     Total params\n",
            "17.189    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:50<00:09,  1.59it/s, loss=3.11, v_num=72mk, val_loss=4.790, val_cer=0.998]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:32<00:00,  1.02it/s, loss=3.03, v_num=72mk, val_loss=2.830, val_cer=0.914]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.55, v_num=72mk, val_loss=2.830, val_cer=0.914]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.5, v_num=72mk, val_loss=2.420, val_cer=0.800] \n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:52<00:09,  1.51it/s, loss=2.41, v_num=72mk, val_loss=2.420, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.37, v_num=72mk, val_loss=2.270, val_cer=0.821]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.32, v_num=72mk, val_loss=2.270, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.3, v_num=72mk, val_loss=2.200, val_cer=0.793] \n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.26, v_num=72mk, val_loss=2.200, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.26, v_num=72mk, val_loss=2.130, val_cer=0.792]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.22, v_num=72mk, val_loss=2.130, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.22, v_num=72mk, val_loss=2.110, val_cer=0.791]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.19, v_num=72mk, val_loss=2.110, val_cer=0.791]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.17, v_num=72mk, val_loss=2.070, val_cer=0.794]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.50it/s, loss=2.16, v_num=72mk, val_loss=2.070, val_cer=0.794]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.16, v_num=72mk, val_loss=2.050, val_cer=0.791]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:09,  1.51it/s, loss=2.13, v_num=72mk, val_loss=2.050, val_cer=0.791]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:35<00:00,  1.01s/it, loss=2.14, v_num=72mk, val_loss=2.030, val_cer=0.799]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:52<00:09,  1.51it/s, loss=2.12, v_num=72mk, val_loss=2.030, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:35<00:00,  1.01s/it, loss=2.11, v_num=72mk, val_loss=2.010, val_cer=0.792]\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.11, v_num=72mk, val_loss=2.010, val_cer=0.792]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:39<00:00,  2.45s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.7907575964927673}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 1108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615336114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 2.12171\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 790\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 2.01239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.79174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.79076\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 179 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbreezy-glitter-1\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/1lbo72mk\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2f2zuxUaJzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba11cbb-2b87-442c-ffe9-4bf4308fec1f"
      },
      "source": [
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-m86ur3wn\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-m86ur3wn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (0.8.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (0.18.2)\n",
            "Collecting PyYAML!=5.4.*,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 276kB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.7.4.post0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.3.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (20.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.4.8)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.3.0.dev0-cp37-none-any.whl size=829805 sha256=767676c1cff1d76165fa3c2216176d27a9bce502f1a61263d18c4178cc05510f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-57sx947m/wheels/e2/c6/88/caa5d4cfbfab631fc84b0107896a6f661a1caf589160c27e71\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=0e2e213c5402222795d3ac921015a86c75d2f67c845f4556662098e84528c0bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML, pytorch-lightning\n",
            "  Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Found existing installation: pytorch-lightning 1.1.4\n",
            "    Uninstalling pytorch-lightning-1.1.4:\n",
            "      Successfully uninstalled pytorch-lightning-1.1.4\n",
            "Successfully installed PyYAML-5.3.1 pytorch-lightning-1.3.0.dev0\n",
            "1.3.0dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSohD0bedq4F",
        "outputId": "66a068c9-1ebb-4019-b766-e5937f26e6ab"
      },
      "source": [
        "!wandb sweep training/sweeps/emnist_lines2_line_cnn_transformer.yml"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: training/sweeps/emnist_lines2_line_cnn_transformer.yml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33mgt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent lucas-spangher/fsdl_lab_5/gt2t31i7\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZNcu4OsigLK",
        "outputId": "3786d504-75d5-4da0-fae3-dea3388f7bf9"
      },
      "source": [
        "!wandb agent lucas-spangher/fsdl_lab_5/gt2t31i7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
            "2021-03-10 00:30:12,917 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-03-10 00:30:13,097 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 00:30:13,097 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.001\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 4\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 00:30:13,099 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.001 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=4 --tf_nhead=8 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 00:30:15.614888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/z62dg7ms\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 00:30:18,111 - wandb.wandb_agent - INFO - Running runs: ['z62dg7ms']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 2.7 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.1 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 764 K \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 459 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 459 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33  | model.embedding                                            | Embedding               | 10.6 K\n",
            "34  | model.fc                                                   | Linear                  | 10.7 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 1.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 1.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 396 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 66.0 K\n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 132 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 131 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 256   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 256   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 256   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 396 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 66.0 K\n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 132 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 131 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 256   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 256   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 256   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "2.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.7 M     Total params\n",
            "10.794    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=2.74, v_num=g7ms, val_loss=4.590, val_cer=0.979]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.33it/s, loss=2.74, v_num=g7ms, val_loss=4.590, val_cer=0.979]\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.11it/s, loss=2.6, v_num=g7ms, val_loss=2.530, val_cer=0.811] \n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=2.48, v_num=g7ms, val_loss=2.530, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=2.48, v_num=g7ms, val_loss=2.530, val_cer=0.811]\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.45, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=2.38, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=2.38, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.08it/s, loss=2.36, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=2.32, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=2.32, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.29, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=2.26, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.29it/s, loss=2.26, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.25, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=2.21, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.23it/s, loss=2.21, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=2.21, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=2.19, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=2.19, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.18, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=2.17, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.32it/s, loss=2.17, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.16, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=2.14, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.14, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=2.14, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=2.12, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:06<00:12,  1.21it/s, loss=2.12, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=2.11, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=2.08, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=2.08, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.09, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=2.08, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.23it/s, loss=2.08, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.38it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=2.05, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=2.04, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.30it/s, loss=2.04, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=2.06, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:47<00:08,  1.69it/s, loss=2.04, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.24it/s, loss=2.04, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:28<00:00,  1.08it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.21it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=2.04, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.70it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.37it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.08it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=2.01, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=2.01, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:28<00:00,  1.08it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]   \n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.70it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.29it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]\n",
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:28<00:00,  1.08it/s, loss=2.01, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:47<00:08,  1.70it/s, loss=1.99, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.99, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.08it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.22it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.36it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.30it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.95, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.24it/s, loss=1.95, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.98, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.96, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=1.96, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.96, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.24it/s, loss=1.96, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.92, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=1.92, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.93, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.33it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.91, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=1.91, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.9, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.40it/s, loss=1.9, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.11it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.9, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.23it/s, loss=1.9, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:11,  1.36it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.11it/s, loss=1.9, v_num=g7ms, val_loss=1.810, val_cer=0.792] \n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.42it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.792]\n",
            "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.77it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.11it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.24it/s, loss=1.88, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.38it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.09it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.25it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.76it/s, loss=1.89, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=1.89, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.11it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.30it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.77it/s, loss=1.87, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.44it/s, loss=1.87, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:25<00:00,  1.10it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=1.86, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.86, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.09it/s, loss=1.86, v_num=g7ms, val_loss=1.780, val_cer=0.795]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:27<00:00,  1.08it/s, loss=1.86, v_num=g7ms, val_loss=1.780, val_cer=0.795]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.12s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.7965301275253296}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 4396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615340610\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 1.75214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 1.78405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.79481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.79653\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcool-sweep-1\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/z62dg7ms\u001b[0m\n",
            "2021-03-10 01:43:38,863 - wandb.wandb_agent - INFO - Cleaning up finished run: z62dg7ms\n",
            "2021-03-10 01:43:40,748 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 01:43:40,748 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 2\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 01:43:40,750 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=2 --tf_nhead=8 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 01:43:43.183716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdifferent-sweep-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/n5egke13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 01:43:45,759 - wandb.wandb_agent - INFO - Running runs: ['n5egke13']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name                                                       | Type                    | Params\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "0  | model                                                      | LineCNNTransformer      | 4.2 M \n",
            "1  | model.line_cnn                                             | LineCNN                 | 3.4 M \n",
            "2  | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3  | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4  | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5  | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6  | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7  | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8  | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9  | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10 | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11 | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12 | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13 | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14 | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15 | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16 | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17 | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18 | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19 | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20 | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21 | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22 | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23 | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24 | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25 | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26 | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27 | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28 | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29 | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30 | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31 | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32 | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33 | model.embedding                                            | Embedding               | 10.6 K\n",
            "34 | model.fc                                                   | Linear                  | 10.7 K\n",
            "35 | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36 | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37 | model.transformer_decoder                                  | TransformerDecoder      | 792 K \n",
            "38 | model.transformer_decoder.layers                           | ModuleList              | 792 K \n",
            "39 | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40 | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41 | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42 | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43 | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44 | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45 | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46 | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47 | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48 | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49 | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50 | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51 | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52 | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53 | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54 | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55 | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56 | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57 | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58 | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59 | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60 | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61 | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62 | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63 | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64 | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65 | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66 | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67 | train_acc                                                  | Accuracy                | 0     \n",
            "68 | val_acc                                                    | Accuracy                | 0     \n",
            "69 | test_acc                                                   | Accuracy                | 0     \n",
            "70 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "71 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "72 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "4.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.2 M     Total params\n",
            "16.782    Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=2.87, v_num=ke13, val_loss=4.800, val_cer=0.990]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:35<00:00,  1.01s/it, loss=2.74, v_num=ke13, val_loss=2.640, val_cer=0.811]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.29it/s, loss=2.6, v_num=ke13, val_loss=2.640, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.01s/it, loss=2.57, v_num=ke13, val_loss=2.480, val_cer=0.766]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=2.49, v_num=ke13, val_loss=2.480, val_cer=0.766]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.02s/it, loss=2.48, v_num=ke13, val_loss=2.400, val_cer=0.770]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.29it/s, loss=2.44, v_num=ke13, val_loss=2.400, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.02s/it, loss=2.44, v_num=ke13, val_loss=2.350, val_cer=0.770]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=2.4, v_num=ke13, val_loss=2.350, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:36<00:00,  1.02s/it, loss=2.39, v_num=ke13, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.35, v_num=ke13, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.35, v_num=ke13, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=2.32, v_num=ke13, val_loss=2.240, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.32, v_num=ke13, val_loss=2.230, val_cer=0.772]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=2.28, v_num=ke13, val_loss=2.230, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.28, v_num=ke13, val_loss=2.170, val_cer=0.774]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.25, v_num=ke13, val_loss=2.170, val_cer=0.774]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=2.26, v_num=ke13, val_loss=2.160, val_cer=0.771]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=2.23, v_num=ke13, val_loss=2.160, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.23, v_num=ke13, val_loss=2.120, val_cer=0.770]\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.22, v_num=ke13, val_loss=2.120, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=2.2, v_num=ke13, val_loss=2.080, val_cer=0.768] \n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.17, v_num=ke13, val_loss=2.080, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=2.18, v_num=ke13, val_loss=2.060, val_cer=0.768]\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=2.16, v_num=ke13, val_loss=2.060, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.16, v_num=ke13, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=2.13, v_num=ke13, val_loss=2.030, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.13, v_num=ke13, val_loss=1.990, val_cer=0.760]\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=2.1, v_num=ke13, val_loss=1.990, val_cer=0.760]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=2.1, v_num=ke13, val_loss=1.980, val_cer=0.759]\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=2.08, v_num=ke13, val_loss=1.980, val_cer=0.759]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.08, v_num=ke13, val_loss=1.960, val_cer=0.749]\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=2.04, v_num=ke13, val_loss=1.960, val_cer=0.749]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=2.02, v_num=ke13, val_loss=1.900, val_cer=0.734]\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.01, v_num=ke13, val_loss=1.900, val_cer=0.734]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=2, v_num=ke13, val_loss=1.860, val_cer=0.724]   \n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.98, v_num=ke13, val_loss=1.860, val_cer=0.724]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.96, v_num=ke13, val_loss=1.790, val_cer=0.711]\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=1.92, v_num=ke13, val_loss=1.790, val_cer=0.711]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=1.92, v_num=ke13, val_loss=1.720, val_cer=0.686]\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=1.86, v_num=ke13, val_loss=1.720, val_cer=0.686]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=1.85, v_num=ke13, val_loss=1.650, val_cer=0.658]\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.8, v_num=ke13, val_loss=1.650, val_cer=0.658]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.79, v_num=ke13, val_loss=1.580, val_cer=0.650]\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=1.73, v_num=ke13, val_loss=1.580, val_cer=0.650]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=1.73, v_num=ke13, val_loss=1.470, val_cer=0.601]\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.64, v_num=ke13, val_loss=1.470, val_cer=0.601]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=1.64, v_num=ke13, val_loss=1.380, val_cer=0.567]\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.59, v_num=ke13, val_loss=1.380, val_cer=0.567]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.57, v_num=ke13, val_loss=1.270, val_cer=0.538]\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.51, v_num=ke13, val_loss=1.270, val_cer=0.538]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.5, v_num=ke13, val_loss=1.200, val_cer=0.508] \n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.42, v_num=ke13, val_loss=1.200, val_cer=0.508]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.41, v_num=ke13, val_loss=1.090, val_cer=0.462]\n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.36, v_num=ke13, val_loss=1.090, val_cer=0.462]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.33, v_num=ke13, val_loss=1.000, val_cer=0.434]\n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=1.29, v_num=ke13, val_loss=1.000, val_cer=0.434]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.26, v_num=ke13, val_loss=0.926, val_cer=0.402]\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=1.22, v_num=ke13, val_loss=0.926, val_cer=0.402]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.2, v_num=ke13, val_loss=0.849, val_cer=0.372] \n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=1.17, v_num=ke13, val_loss=0.849, val_cer=0.372]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=1.15, v_num=ke13, val_loss=0.800, val_cer=0.352]\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.1, v_num=ke13, val_loss=0.800, val_cer=0.352]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=1.11, v_num=ke13, val_loss=0.778, val_cer=0.344]\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=1.05, v_num=ke13, val_loss=0.778, val_cer=0.344]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=1.04, v_num=ke13, val_loss=0.726, val_cer=0.311]\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=1, v_num=ke13, val_loss=0.726, val_cer=0.311]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:39<00:00,  1.04s/it, loss=1.01, v_num=ke13, val_loss=0.678, val_cer=0.294]\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.967, v_num=ke13, val_loss=0.678, val_cer=0.294]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=0.956, v_num=ke13, val_loss=0.650, val_cer=0.279]\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.94, v_num=ke13, val_loss=0.650, val_cer=0.279]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.917, v_num=ke13, val_loss=0.605, val_cer=0.252]\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.899, v_num=ke13, val_loss=0.605, val_cer=0.252]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=0.904, v_num=ke13, val_loss=0.604, val_cer=0.259]\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=0.86, v_num=ke13, val_loss=0.604, val_cer=0.259]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.846, v_num=ke13, val_loss=0.556, val_cer=0.239]\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=0.82, v_num=ke13, val_loss=0.556, val_cer=0.239]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.02s/it, loss=0.805, v_num=ke13, val_loss=0.560, val_cer=0.227]\n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.823, v_num=ke13, val_loss=0.560, val_cer=0.227]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.799, v_num=ke13, val_loss=0.513, val_cer=0.219]\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=0.778, v_num=ke13, val_loss=0.513, val_cer=0.219]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=0.785, v_num=ke13, val_loss=0.525, val_cer=0.219]\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=0.746, v_num=ke13, val_loss=0.525, val_cer=0.219]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=0.738, v_num=ke13, val_loss=0.461, val_cer=0.197]\n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.735, v_num=ke13, val_loss=0.461, val_cer=0.197]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.726, v_num=ke13, val_loss=0.473, val_cer=0.203]\n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.734, v_num=ke13, val_loss=0.473, val_cer=0.203]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=0.716, v_num=ke13, val_loss=0.435, val_cer=0.179]\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=0.692, v_num=ke13, val_loss=0.435, val_cer=0.179]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.692, v_num=ke13, val_loss=0.419, val_cer=0.173]\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=0.68, v_num=ke13, val_loss=0.419, val_cer=0.173]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=0.658, v_num=ke13, val_loss=0.407, val_cer=0.169]\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=0.662, v_num=ke13, val_loss=0.407, val_cer=0.169]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.03s/it, loss=0.635, v_num=ke13, val_loss=0.412, val_cer=0.170]\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=0.631, v_num=ke13, val_loss=0.412, val_cer=0.170]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=0.628, v_num=ke13, val_loss=0.392, val_cer=0.159]\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.25it/s, loss=0.63, v_num=ke13, val_loss=0.392, val_cer=0.159]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:38<00:00,  1.04s/it, loss=0.633, v_num=ke13, val_loss=0.431, val_cer=0.167]\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.27it/s, loss=0.625, v_num=ke13, val_loss=0.431, val_cer=0.167]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=0.612, v_num=ke13, val_loss=0.367, val_cer=0.150]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:37<00:00,  1.03s/it, loss=0.612, v_num=ke13, val_loss=0.367, val_cer=0.150]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:23<00:00,  1.49s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.1210828348994255}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 9821\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 4945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615345567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.60932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.36696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.14975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.12108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdifferent-sweep-2\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/n5egke13\u001b[0m\n",
            "2021-03-10 03:06:15,923 - wandb.wandb_agent - INFO - Cleaning up finished run: n5egke13\n",
            "2021-03-10 03:06:16,134 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 03:06:16,135 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 2\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 03:06:16,137 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=2 --tf_nhead=4 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 03:06:18.463692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-sweep-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/48whs9dq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 03:06:21,145 - wandb.wandb_agent - INFO - Running runs: ['48whs9dq']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name                                                       | Type                    | Params\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "0  | model                                                      | LineCNNTransformer      | 3.2 M \n",
            "1  | model.line_cnn                                             | LineCNN                 | 2.4 M \n",
            "2  | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3  | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4  | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5  | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6  | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7  | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8  | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9  | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10 | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11 | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12 | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13 | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14 | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15 | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16 | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17 | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18 | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19 | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20 | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21 | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22 | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23 | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24 | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25 | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26 | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27 | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28 | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29 | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30 | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31 | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32 | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33 | model.embedding                                            | Embedding               | 10.6 K\n",
            "34 | model.fc                                                   | Linear                  | 10.7 K\n",
            "35 | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36 | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37 | model.transformer_decoder                                  | TransformerDecoder      | 792 K \n",
            "38 | model.transformer_decoder.layers                           | ModuleList              | 792 K \n",
            "39 | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40 | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41 | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42 | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43 | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44 | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45 | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46 | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47 | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48 | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49 | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50 | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51 | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52 | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53 | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54 | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55 | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56 | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57 | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58 | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59 | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60 | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61 | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62 | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63 | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64 | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65 | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66 | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67 | train_acc                                                  | Accuracy                | 0     \n",
            "68 | val_acc                                                    | Accuracy                | 0     \n",
            "69 | test_acc                                                   | Accuracy                | 0     \n",
            "70 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "71 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "72 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.872    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.83it/s, loss=2.84, v_num=s9dq, val_loss=4.510, val_cer=0.980]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.33it/s, loss=2.84, v_num=s9dq, val_loss=4.510, val_cer=0.980]\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.72, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.57, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=2.57, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.54, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.48, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.43it/s, loss=2.48, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.46, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.43, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:11,  1.36it/s, loss=2.43, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.43, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.4, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.30it/s, loss=2.4, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.41it/s, loss=2.39, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.36, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:54<00:10,  1.47it/s, loss=2.36, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.34, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.31, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.40it/s, loss=2.31, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.31, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.3, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=2.3, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.29, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.26, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=2.26, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.25, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.42it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.83it/s, loss=2.22, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=2.22, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.40it/s, loss=2.2, v_num=s9dq, val_loss=2.090, val_cer=0.768] \n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.81it/s, loss=2.18, v_num=s9dq, val_loss=2.090, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.30it/s, loss=2.18, v_num=s9dq, val_loss=2.090, val_cer=0.768]\n",
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.39it/s, loss=2.19, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.82it/s, loss=2.16, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=2.16, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.39it/s, loss=2.16, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.81it/s, loss=2.13, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:54<00:10,  1.47it/s, loss=2.13, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.39it/s, loss=2.12, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.82it/s, loss=2.11, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.42it/s, loss=2.11, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.40it/s, loss=2.12, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.80it/s, loss=2.09, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.38it/s, loss=2.09, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.39it/s, loss=2.08, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:44<00:08,  1.82it/s, loss=2.07, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=2.07, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:08<00:00,  1.40it/s, loss=2.05, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.86it/s, loss=2.04, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=2.04, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=2.02, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:42<00:08,  1.86it/s, loss=1.99, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:54<00:10,  1.47it/s, loss=1.99, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=1.99, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.95, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.38it/s, loss=1.95, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.95, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.91, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.91, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.9, v_num=s9dq, val_loss=1.690, val_cer=0.681] \n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.86, v_num=s9dq, val_loss=1.690, val_cer=0.681]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:54<00:10,  1.47it/s, loss=1.86, v_num=s9dq, val_loss=1.690, val_cer=0.681]\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.85, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=1.81, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.40it/s, loss=1.81, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.81, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.76, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=1.76, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=1.76, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.7, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:10,  1.49it/s, loss=1.7, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=1.69, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.86it/s, loss=1.67, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=1.67, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=1.65, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=1.59, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=1.59, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.41it/s, loss=1.6, v_num=s9dq, val_loss=1.270, val_cer=0.536] \n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.54, v_num=s9dq, val_loss=1.270, val_cer=0.536]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=1.54, v_num=s9dq, val_loss=1.270, val_cer=0.536]\n",
            "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.54, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.48, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.43it/s, loss=1.48, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=1.46, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.45, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=1.45, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.44, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.39, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.29it/s, loss=1.39, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.38, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=1.35, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.45it/s, loss=1.35, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.33, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.83it/s, loss=1.32, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.37it/s, loss=1.32, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.41it/s, loss=1.31, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=1.28, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.31it/s, loss=1.28, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.26, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=1.24, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:10,  1.48it/s, loss=1.24, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.23, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.2, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=1.2, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.22, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.16, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.33it/s, loss=1.16, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.16, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.15, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:53<00:10,  1.50it/s, loss=1.15, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.13, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=1.09, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.42it/s, loss=1.09, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=1.09, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.83it/s, loss=1.07, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.34it/s, loss=1.07, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.41it/s, loss=1.06, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.82it/s, loss=1.04, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.29it/s, loss=1.04, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.40it/s, loss=1.03, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.82it/s, loss=1.04, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:54<00:10,  1.47it/s, loss=1.04, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.41it/s, loss=1, v_num=s9dq, val_loss=0.700, val_cer=0.297]   \n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.83it/s, loss=1.01, v_num=s9dq, val_loss=0.700, val_cer=0.297]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=1.01, v_num=s9dq, val_loss=0.700, val_cer=0.297]\n",
            "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:07<00:00,  1.42it/s, loss=1, v_num=s9dq, val_loss=0.665, val_cer=0.289]   \n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=0.994, v_num=s9dq, val_loss=0.665, val_cer=0.289]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.34it/s, loss=0.994, v_num=s9dq, val_loss=0.665, val_cer=0.289]\n",
            "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=0.986, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=0.961, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=0.961, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=0.947, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=0.945, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.43it/s, loss=0.945, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=0.928, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:42<00:08,  1.86it/s, loss=0.913, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=0.913, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.43it/s, loss=0.916, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=0.912, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.28it/s, loss=0.912, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=0.901, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.84it/s, loss=0.878, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:55<00:10,  1.43it/s, loss=0.878, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=0.889, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:43<00:08,  1.85it/s, loss=0.869, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:11,  1.36it/s, loss=0.869, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=0.878, v_num=s9dq, val_loss=0.555, val_cer=0.245]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:06<00:00,  1.42it/s, loss=0.878, v_num=s9dq, val_loss=0.555, val_cer=0.245]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.09s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.1871183067560196}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 18664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 3391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615348968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.83147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.55534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.2448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.18712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfancy-sweep-3\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/48whs9dq\u001b[0m\n",
            "2021-03-10 04:02:55,114 - wandb.wandb_agent - INFO - Cleaning up finished run: 48whs9dq\n",
            "2021-03-10 04:02:57,333 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 04:02:57,333 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 6\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 04:02:57,335 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=6 --tf_nhead=8 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 04:02:59.701015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-sweep-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/v3bw7ha9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 04:03:02,342 - wandb.wandb_agent - INFO - Running runs: ['v3bw7ha9']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 5.8 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 3.4 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33  | model.embedding                                            | Embedding               | 10.6 K\n",
            "34  | model.fc                                                   | Linear                  | 10.7 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.4 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.4 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 396 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 66.0 K\n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 132 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 131 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 256   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 256   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 256   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 396 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 66.0 K\n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 132 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 131 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 256   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 256   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 256   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | model.transformer_decoder.layers.4                         | TransformerDecoderLayer | 396 K \n",
            "96  | model.transformer_decoder.layers.4.self_attn               | MultiheadAttention      | 66.0 K\n",
            "97  | model.transformer_decoder.layers.4.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "98  | model.transformer_decoder.layers.4.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "99  | model.transformer_decoder.layers.4.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "100 | model.transformer_decoder.layers.4.linear1                 | Linear                  | 132 K \n",
            "101 | model.transformer_decoder.layers.4.dropout                 | Dropout                 | 0     \n",
            "102 | model.transformer_decoder.layers.4.linear2                 | Linear                  | 131 K \n",
            "103 | model.transformer_decoder.layers.4.norm1                   | LayerNorm               | 256   \n",
            "104 | model.transformer_decoder.layers.4.norm2                   | LayerNorm               | 256   \n",
            "105 | model.transformer_decoder.layers.4.norm3                   | LayerNorm               | 256   \n",
            "106 | model.transformer_decoder.layers.4.dropout1                | Dropout                 | 0     \n",
            "107 | model.transformer_decoder.layers.4.dropout2                | Dropout                 | 0     \n",
            "108 | model.transformer_decoder.layers.4.dropout3                | Dropout                 | 0     \n",
            "109 | model.transformer_decoder.layers.5                         | TransformerDecoderLayer | 396 K \n",
            "110 | model.transformer_decoder.layers.5.self_attn               | MultiheadAttention      | 66.0 K\n",
            "111 | model.transformer_decoder.layers.5.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "112 | model.transformer_decoder.layers.5.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "113 | model.transformer_decoder.layers.5.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "114 | model.transformer_decoder.layers.5.linear1                 | Linear                  | 132 K \n",
            "115 | model.transformer_decoder.layers.5.dropout                 | Dropout                 | 0     \n",
            "116 | model.transformer_decoder.layers.5.linear2                 | Linear                  | 131 K \n",
            "117 | model.transformer_decoder.layers.5.norm1                   | LayerNorm               | 256   \n",
            "118 | model.transformer_decoder.layers.5.norm2                   | LayerNorm               | 256   \n",
            "119 | model.transformer_decoder.layers.5.norm3                   | LayerNorm               | 256   \n",
            "Validation sanity check: 0it [00:00, ?it/s]          | 0     \n",
            "121 | model.transformer_decoder.layers.5.dropout2                | Dropout                 | 0     \n",
            "122 | model.transformer_decoder.layers.5.dropout3                | Dropout                 | 0     \n",
            "123 | train_acc                                                  | Accuracy                | 0     \n",
            "124 | val_acc                                                    | Accuracy                | 0     \n",
            "125 | test_acc                                                   | Accuracy                | 0     \n",
            "126 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "127 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "128 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "5.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.8 M     Total params\n",
            "23.121    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=3.14, v_num=7ha9, val_loss=4.720, val_cer=0.999]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.40s/it, loss=3.09, v_num=7ha9, val_loss=2.890, val_cer=0.929]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.65, v_num=7ha9, val_loss=2.890, val_cer=0.929]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.61, v_num=7ha9, val_loss=2.540, val_cer=0.811]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.54, v_num=7ha9, val_loss=2.540, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.52, v_num=7ha9, val_loss=2.460, val_cer=0.821]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.48, v_num=7ha9, val_loss=2.460, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.47, v_num=7ha9, val_loss=2.390, val_cer=0.821]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=2.42, v_num=7ha9, val_loss=2.390, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=2.39, v_num=7ha9, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=2.34, v_num=7ha9, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=2.33, v_num=7ha9, val_loss=2.220, val_cer=0.768]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=2.3, v_num=7ha9, val_loss=2.220, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=2.29, v_num=7ha9, val_loss=2.190, val_cer=0.771]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=2.25, v_num=7ha9, val_loss=2.190, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=2.26, v_num=7ha9, val_loss=2.150, val_cer=0.772]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.22, v_num=7ha9, val_loss=2.150, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.22, v_num=7ha9, val_loss=2.120, val_cer=0.770]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.2, v_num=7ha9, val_loss=2.120, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.2, v_num=7ha9, val_loss=2.100, val_cer=0.771]\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.19, v_num=7ha9, val_loss=2.100, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.17, v_num=7ha9, val_loss=2.070, val_cer=0.775]\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.14, v_num=7ha9, val_loss=2.070, val_cer=0.775]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:13<00:00,  1.41s/it, loss=2.15, v_num=7ha9, val_loss=2.050, val_cer=0.773]\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=2.13, v_num=7ha9, val_loss=2.050, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.13, v_num=7ha9, val_loss=2.020, val_cer=0.779]\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.11, v_num=7ha9, val_loss=2.020, val_cer=0.779]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.12, v_num=7ha9, val_loss=2.000, val_cer=0.781]\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.14it/s, loss=2.09, v_num=7ha9, val_loss=2.000, val_cer=0.781]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.08, v_num=7ha9, val_loss=1.980, val_cer=0.781]\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.07, v_num=7ha9, val_loss=1.980, val_cer=0.781]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.06, v_num=7ha9, val_loss=1.950, val_cer=0.786]\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.05, v_num=7ha9, val_loss=1.950, val_cer=0.786]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.03, v_num=7ha9, val_loss=1.930, val_cer=0.779]\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.15it/s, loss=2.02, v_num=7ha9, val_loss=1.930, val_cer=0.779]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2.02, v_num=7ha9, val_loss=1.910, val_cer=0.778]\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.14it/s, loss=2.02, v_num=7ha9, val_loss=1.910, val_cer=0.778]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=2, v_num=7ha9, val_loss=1.880, val_cer=0.775]   \n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.98, v_num=7ha9, val_loss=1.880, val_cer=0.775]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=1.99, v_num=7ha9, val_loss=1.880, val_cer=0.771]\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.14it/s, loss=1.96, v_num=7ha9, val_loss=1.880, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.95, v_num=7ha9, val_loss=1.840, val_cer=0.776]\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.14it/s, loss=1.94, v_num=7ha9, val_loss=1.840, val_cer=0.776]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.41s/it, loss=1.94, v_num=7ha9, val_loss=1.840, val_cer=0.767]\n",
            "Epoch 22:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.92, v_num=7ha9, val_loss=1.840, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.93, v_num=7ha9, val_loss=1.790, val_cer=0.766]\n",
            "Epoch 23:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.89, v_num=7ha9, val_loss=1.790, val_cer=0.766]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.89, v_num=7ha9, val_loss=1.780, val_cer=0.752]\n",
            "Epoch 24:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.9, v_num=7ha9, val_loss=1.780, val_cer=0.752]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.88, v_num=7ha9, val_loss=1.750, val_cer=0.741]\n",
            "Epoch 25:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.86, v_num=7ha9, val_loss=1.750, val_cer=0.741]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.85, v_num=7ha9, val_loss=1.720, val_cer=0.736]\n",
            "Epoch 26:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.83, v_num=7ha9, val_loss=1.720, val_cer=0.736]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.83, v_num=7ha9, val_loss=1.690, val_cer=0.729]\n",
            "Epoch 27:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.82, v_num=7ha9, val_loss=1.690, val_cer=0.729]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.8, v_num=7ha9, val_loss=1.680, val_cer=0.717] \n",
            "Epoch 28:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.78, v_num=7ha9, val_loss=1.680, val_cer=0.717]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=1.78, v_num=7ha9, val_loss=1.610, val_cer=0.698]\n",
            "Epoch 29:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.75, v_num=7ha9, val_loss=1.610, val_cer=0.698]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.72, v_num=7ha9, val_loss=1.570, val_cer=0.686]\n",
            "Epoch 30:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.7, v_num=7ha9, val_loss=1.570, val_cer=0.686]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.67, v_num=7ha9, val_loss=1.510, val_cer=0.666]\n",
            "Epoch 31:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.64, v_num=7ha9, val_loss=1.510, val_cer=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=1.63, v_num=7ha9, val_loss=1.440, val_cer=0.655]\n",
            "Epoch 32:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.58, v_num=7ha9, val_loss=1.440, val_cer=0.655]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.56, v_num=7ha9, val_loss=1.350, val_cer=0.618]\n",
            "Epoch 33:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.52, v_num=7ha9, val_loss=1.350, val_cer=0.618]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.51, v_num=7ha9, val_loss=1.300, val_cer=0.597]\n",
            "Epoch 34:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.46, v_num=7ha9, val_loss=1.300, val_cer=0.597]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.43, v_num=7ha9, val_loss=1.220, val_cer=0.567]\n",
            "Epoch 35:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.4, v_num=7ha9, val_loss=1.220, val_cer=0.567]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.39, v_num=7ha9, val_loss=1.180, val_cer=0.534]\n",
            "Epoch 36:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.34, v_num=7ha9, val_loss=1.180, val_cer=0.534]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.34, v_num=7ha9, val_loss=1.120, val_cer=0.515]\n",
            "Epoch 37:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.28, v_num=7ha9, val_loss=1.120, val_cer=0.515]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.27, v_num=7ha9, val_loss=0.999, val_cer=0.476]\n",
            "Epoch 38:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.22, v_num=7ha9, val_loss=0.999, val_cer=0.476]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.22, v_num=7ha9, val_loss=0.956, val_cer=0.454]\n",
            "Epoch 39:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.19, v_num=7ha9, val_loss=0.956, val_cer=0.454]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.17, v_num=7ha9, val_loss=0.912, val_cer=0.431]\n",
            "Epoch 40:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.14, v_num=7ha9, val_loss=0.912, val_cer=0.431]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=1.15, v_num=7ha9, val_loss=0.855, val_cer=0.418]\n",
            "Epoch 41:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.14it/s, loss=1.08, v_num=7ha9, val_loss=0.855, val_cer=0.418]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.42s/it, loss=1.07, v_num=7ha9, val_loss=0.812, val_cer=0.390]\n",
            "Epoch 42:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:09<00:13,  1.14it/s, loss=1.07, v_num=7ha9, val_loss=0.812, val_cer=0.390]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:14<00:00,  1.42s/it, loss=1.05, v_num=7ha9, val_loss=0.811, val_cer=0.388]\n",
            "Epoch 43:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=1.02, v_num=7ha9, val_loss=0.811, val_cer=0.388]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=1.01, v_num=7ha9, val_loss=0.733, val_cer=0.352]\n",
            "Epoch 44:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=0.999, v_num=7ha9, val_loss=0.733, val_cer=0.352]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:16<00:00,  1.43s/it, loss=0.983, v_num=7ha9, val_loss=0.733, val_cer=0.358]\n",
            "Epoch 45:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=0.96, v_num=7ha9, val_loss=0.733, val_cer=0.358]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:16<00:00,  1.44s/it, loss=0.933, v_num=7ha9, val_loss=0.689, val_cer=0.337]\n",
            "Epoch 46:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=0.919, v_num=7ha9, val_loss=0.689, val_cer=0.337]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:16<00:00,  1.44s/it, loss=0.897, v_num=7ha9, val_loss=0.655, val_cer=0.311]\n",
            "Epoch 47:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=0.888, v_num=7ha9, val_loss=0.655, val_cer=0.311]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:16<00:00,  1.44s/it, loss=0.906, v_num=7ha9, val_loss=0.666, val_cer=0.304]\n",
            "Epoch 48:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=0.853, v_num=7ha9, val_loss=0.666, val_cer=0.304]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:16<00:00,  1.44s/it, loss=0.861, v_num=7ha9, val_loss=0.597, val_cer=0.291]\n",
            "Epoch 49:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=0.846, v_num=7ha9, val_loss=0.597, val_cer=0.291]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=0.836, v_num=7ha9, val_loss=0.602, val_cer=0.282]\n",
            "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:15<00:00,  1.43s/it, loss=0.836, v_num=7ha9, val_loss=0.602, val_cer=0.282]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:53<00:00,  3.33s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.24399620294570923}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 27509\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 6828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615355806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.74084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.60189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.28191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msnowy-sweep-4\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/v3bw7ha9\u001b[0m\n",
            "2021-03-10 05:56:53,062 - wandb.wandb_agent - INFO - Cleaning up finished run: v3bw7ha9\n",
            "2021-03-10 05:56:53,685 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 05:56:53,685 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.01\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 256\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 4\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 05:56:53,687 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.01 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=256 --tf_fc_dim=1024 --tf_layers=4 --tf_nhead=4 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 05:56:56.218414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-sweep-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/5iljkf8u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 05:56:58,697 - wandb.wandb_agent - INFO - Running runs: ['5iljkf8u']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 7.7 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 3.5 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 2.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 262 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 4.2 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 4.2 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 1.1 M \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 263 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 262 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 1.1 M \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 263 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 262 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 1.1 M \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 263 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 262 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 1.1 M \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 263 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 262 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "7.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.7 M     Total params\n",
            "30.837    Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=4.940, val_cer=0.999]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.33it/s, loss=nan, v_num=kf8u, val_loss=4.940, val_cer=0.999]\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.37it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:39<00:00,  1.05s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.37it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.38it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.21it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:04<00:12,  1.24it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:02<00:11,  1.27it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.29it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.31it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:57<00:10,  1.39it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.40it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.22it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:05<00:12,  1.23it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:03<00:11,  1.26it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:01<00:11,  1.29it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:00<00:11,  1.32it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:59<00:11,  1.35it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:58<00:10,  1.38it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [00:56<00:10,  1.41it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:48<00:00,  3.00s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.9744817018508911}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 36351\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 _runtime 2291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               _timestamp 1615358106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    _step 426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               train_loss nan\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      trainer/global_step 1738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 val_loss 3.2239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  val_cer 0.97465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 test_cer 0.97448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 371 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mblooming-sweep-5\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/5iljkf8u\u001b[0m\n",
            "2021-03-10 06:35:12,608 - wandb.wandb_agent - INFO - Cleaning up finished run: 5iljkf8u\n",
            "2021-03-10 06:35:21,668 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 06:35:21,669 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.01\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 256\n",
            "\ttf_fc_dim: 256\n",
            "\ttf_layers: 6\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 06:35:21,671 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.01 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=256 --tf_fc_dim=256 --tf_layers=6 --tf_nhead=4 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 06:35:24.057980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msparkling-sweep-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/pzk81bv9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_063523-pzk81bv9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 06:35:26,681 - wandb.wandb_agent - INFO - Running runs: ['pzk81bv9']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 8.4 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 4.4 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 262 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 4.0 M \n",
            "Validation sanity check: 0it [00:00, ?it/s]  | 4.0 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]      | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | model.transformer_decoder.layers.4                         | TransformerDecoderLayer | 659 K \n",
            "96  | model.transformer_decoder.layers.4.self_attn               | MultiheadAttention      | 263 K \n",
            "97  | model.transformer_decoder.layers.4.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "98  | model.transformer_decoder.layers.4.multihead_attn          | MultiheadAttention      | 263 K \n",
            "99  | model.transformer_decoder.layers.4.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "100 | model.transformer_decoder.layers.4.linear1                 | Linear                  | 65.8 K\n",
            "101 | model.transformer_decoder.layers.4.dropout                 | Dropout                 | 0     \n",
            "102 | model.transformer_decoder.layers.4.linear2                 | Linear                  | 65.8 K\n",
            "103 | model.transformer_decoder.layers.4.norm1                   | LayerNorm               | 512   \n",
            "104 | model.transformer_decoder.layers.4.norm2                   | LayerNorm               | 512   \n",
            "105 | model.transformer_decoder.layers.4.norm3                   | LayerNorm               | 512   \n",
            "106 | model.transformer_decoder.layers.4.dropout1                | Dropout                 | 0     \n",
            "107 | model.transformer_decoder.layers.4.dropout2                | Dropout                 | 0     \n",
            "108 | model.transformer_decoder.layers.4.dropout3                | Dropout                 | 0     \n",
            "109 | model.transformer_decoder.layers.5                         | TransformerDecoderLayer | 659 K \n",
            "110 | model.transformer_decoder.layers.5.self_attn               | MultiheadAttention      | 263 K \n",
            "111 | model.transformer_decoder.layers.5.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "112 | model.transformer_decoder.layers.5.multihead_attn          | MultiheadAttention      | 263 K \n",
            "113 | model.transformer_decoder.layers.5.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "114 | model.transformer_decoder.layers.5.linear1                 | Linear                  | 65.8 K\n",
            "115 | model.transformer_decoder.layers.5.dropout                 | Dropout                 | 0     \n",
            "116 | model.transformer_decoder.layers.5.linear2                 | Linear                  | 65.8 K\n",
            "117 | model.transformer_decoder.layers.5.norm1                   | LayerNorm               | 512   \n",
            "118 | model.transformer_decoder.layers.5.norm2                   | LayerNorm               | 512   \n",
            "119 | model.transformer_decoder.layers.5.norm3                   | LayerNorm               | 512   \n",
            "120 | model.transformer_decoder.layers.5.dropout1                | Dropout                 | 0     \n",
            "121 | model.transformer_decoder.layers.5.dropout2                | Dropout                 | 0     \n",
            "122 | model.transformer_decoder.layers.5.dropout3                | Dropout                 | 0     \n",
            "123 | train_acc                                                  | Accuracy                | 0     \n",
            "124 | val_acc                                                    | Accuracy                | 0     \n",
            "125 | test_acc                                                   | Accuracy                | 0     \n",
            "126 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "127 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "128 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "8.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "8.4 M     Total params\n",
            "33.462    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=nan, v_num=1bv9, val_loss=5.120, val_cer=0.998]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:25<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.12it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:25<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 5:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:11<00:13,  1.11it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:26<00:00,  1.54s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 6:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:24<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 7:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 8:   0%|          | 0/95 [00:00<?, ?it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q7smXzQinE3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}