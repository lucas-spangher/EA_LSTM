{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbJRsoxUGSlAqSRmpC1RII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-spangher/EA_LSTM/blob/master/Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ_q2PkVeL2R",
        "outputId": "ea6589f6-061e-43af-9caa-b3bdb8902a29"
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ❯ \u001b[0mlucas-spangher\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0ms\u001b[0mo\u001b[0mc\u001b[0mi\u001b[0ma\u001b[0ml\u001b[0m-\u001b[0mg\u001b[0ma\u001b[0mm\u001b[0me\u001b[0m-\u001b[0mr\u001b[0ml\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0;38;5;214;1m lucas-spangher\u001b[43D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ❯ \u001b[0mfsdl_lab_5\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0mC\u001b[0mr\u001b[0me\u001b[0ma\u001b[0mt\u001b[0me\u001b[0m \u001b[0mN\u001b[0me\u001b[0mw\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0;38;5;214;1m fsdl_lab_5\u001b[42D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"fsdl_lab_5\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlOKhUQ1PpQo",
        "outputId": "bd50e96d-0a99-499f-edb0-1a91390e8d7d"
      },
      "source": [
        "# FSDL Spring 2021 Setup\n",
        "!git clone https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n",
        "%cd fsdl-text-recognizer-2021-labs\n",
        "!pip install boltons pytorch_lightning==1.1.4 wandb\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fsdl-text-recognizer-2021-labs'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 354 (delta 158), reused 296 (delta 109), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 3.83 MiB | 32.70 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n",
            "/content/fsdl-text-recognizer-2021-labs\n",
            "Collecting boltons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e1/e7979a4a6d4b296b5935e926549fff540f7670ddaf09bbf137e2b022c039/boltons-20.2.1-py2.py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 17.7MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning==1.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 43.2MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (4.41.1)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 48.1MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.19.5)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.2MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->pytorch_lightning==1.1.4) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.27.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (3.7.0)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (3.4.1)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (20.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 53.0MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 51.9MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.1.0)\n",
            "Building wheels for collected packages: future, subprocess32, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=f23caeee03130dfec60417a30ddccf3a83b788407df182ab2ec0fa6e8f35a12c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=a7473550cbde761e78dff428d4d610d20ea5084943101bd7eb96c0020c479544\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=f0ceb5e6288b7f97f68705d740d78ff74cfc4e473a8c0ef12eb19f96f91f3eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built future subprocess32 pathtools\n",
            "Installing collected packages: boltons, PyYAML, future, async-timeout, multidict, yarl, aiohttp, fsspec, pytorch-lightning, subprocess32, configparser, sentry-sdk, docker-pycreds, pathtools, shortuuid, smmap, gitdb, GitPython, wandb\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.14 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 boltons-20.2.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-0.8.7 future-0.18.2 gitdb-4.0.5 multidict-5.1.0 pathtools-0.1.2 pytorch-lightning-1.1.4 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21 yarl-1.6.3\n",
            "env: PYTHONPATH=.:$PYTHONPATH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcP36XkzT0OY",
        "outputId": "42920d54-22f6-48a1-bcd1-a3bce89cb720"
      },
      "source": [
        "cd lab5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwvITJRUaGa4"
      },
      "source": [
        "Set up wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo5n3pTtTmoq",
        "outputId": "ea6589f6-061e-43af-9caa-b3bdb8902a29"
      },
      "source": [
        "!wandb init"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ❯ \u001b[0mlucas-spangher\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0ms\u001b[0mo\u001b[0mc\u001b[0mi\u001b[0ma\u001b[0ml\u001b[0m-\u001b[0mg\u001b[0ma\u001b[0mm\u001b[0me\u001b[0m-\u001b[0mr\u001b[0ml\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which team should we use? \u001b[0;38;5;214;1m lucas-spangher\u001b[43D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?1000h\u001b[?1015h\u001b[?1006h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0m (Use arrow keys)\u001b[0m\n",
            "\u001b[0;38;5;214;1m ❯ \u001b[0mfsdl_lab_5\u001b[0m\n",
            "\u001b[0m \u001b[0m \u001b[0m \u001b[0mC\u001b[0mr\u001b[0me\u001b[0ma\u001b[0mt\u001b[0me\u001b[0m \u001b[0mN\u001b[0me\u001b[0mw\u001b[0m\n",
            "\u001b[3A\u001b[?7h\u001b[0m\u001b[?25l\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;67m?\u001b[0;1m Which project should we use? \u001b[0;38;5;214;1m fsdl_lab_5\u001b[42D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1015l\u001b[?1006l\u001b[?2004l\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"fsdl_lab_5\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyMaDanna-Bf",
        "outputId": "31f472bf-1790-42e0-d431-bfab24d2e552"
      },
      "source": [
        "!python training/run_experiment.py --wandb --max_epochs=10 --gpus='0,' --num_workers=4 --data_class=EMNISTLines2 --model_class=LineCNNTransformer --loss=transformer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 00:10:07.722666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-glitter-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/1lbo72mk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "EMNISTLines2 generating data for train...\n",
            "[nltk_data] Downloading package brown to /content/fsdl-text-\n",
            "[nltk_data]     recognizer-2021-labs/data/downloaded/nltk...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "Downloading raw dataset from https://s3-us-west-2.amazonaws.com/fsdl-public-assets/matlab.zip to /content/fsdl-text-recognizer-2021-labs/data/downloaded/emnist/matlab.zip...\n",
            "709MB [00:21, 34.8MB/s]                           \n",
            "Computing SHA-256...\n",
            "Unzipping EMNIST...\n",
            "Loading training data from .mat file\n",
            "Balancing classes to reduce amount of data\n",
            "Saving to HDF5 in a compressed format...\n",
            "Saving essential dataset parameters to text_recognizer/datasets...\n",
            "Cleaning up...\n",
            "tcmalloc: large alloc 2293760000 bytes == 0x55b036866000 @  0x7f3c99674b6b 0x7f3c99694379 0x7f3c382ab25e 0x7f3c382ac9d2 0x7f3c74f6bad6 0x7f3c753cdff9 0x7f3c758d839a 0x7f3c758a3b19 0x7f3c7585a277 0x7f3c756fe549 0x7f3c753d1b2c 0x7f3c759dee29 0x7f3c759df0aa 0x7f3c758b2e59 0x7f3c75866f3e 0x7f3c7573a11b 0x7f3c86aef783 0x55afe1b40050 0x55afe1b3fde0 0x55afe1bb4244 0x55afe1b4169a 0x55afe1bafa45 0x55afe1b4169a 0x55afe1bafc9e 0x55afe1a80d14 0x55afe1bb11e6 0x55afe1baee0d 0x55afe1a80eb0 0x55afe1bb11e6 0x55afe1baee0d 0x55afe1b4177a\n",
            "EMNISTLines2 generating data for val...\n",
            "EMNISTLines2 generating data for test...\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 4.3 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.6 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "4.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.3 M     Total params\n",
            "17.189    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:50<00:09,  1.59it/s, loss=3.11, v_num=72mk, val_loss=4.790, val_cer=0.998]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 95/95 [01:32<00:00,  1.02it/s, loss=3.03, v_num=72mk, val_loss=2.830, val_cer=0.914]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.55, v_num=72mk, val_loss=2.830, val_cer=0.914]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.5, v_num=72mk, val_loss=2.420, val_cer=0.800] \n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:52<00:09,  1.51it/s, loss=2.41, v_num=72mk, val_loss=2.420, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.37, v_num=72mk, val_loss=2.270, val_cer=0.821]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.32, v_num=72mk, val_loss=2.270, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.3, v_num=72mk, val_loss=2.200, val_cer=0.793] \n",
            "Epoch 4:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.26, v_num=72mk, val_loss=2.200, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.26, v_num=72mk, val_loss=2.130, val_cer=0.792]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.22, v_num=72mk, val_loss=2.130, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.22, v_num=72mk, val_loss=2.110, val_cer=0.791]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.19, v_num=72mk, val_loss=2.110, val_cer=0.791]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.17, v_num=72mk, val_loss=2.070, val_cer=0.794]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [00:53<00:09,  1.50it/s, loss=2.16, v_num=72mk, val_loss=2.070, val_cer=0.794]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.16, v_num=72mk, val_loss=2.050, val_cer=0.791]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [00:53<00:09,  1.51it/s, loss=2.13, v_num=72mk, val_loss=2.050, val_cer=0.791]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 95/95 [01:35<00:00,  1.01s/it, loss=2.14, v_num=72mk, val_loss=2.030, val_cer=0.799]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [00:52<00:09,  1.51it/s, loss=2.12, v_num=72mk, val_loss=2.030, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 95/95 [01:35<00:00,  1.01s/it, loss=2.11, v_num=72mk, val_loss=2.010, val_cer=0.792]\n",
            "Epoch 9: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.11, v_num=72mk, val_loss=2.010, val_cer=0.792]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:39<00:00,  2.45s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.7907575964927673}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_001006-1lbo72mk/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 1108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615336114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 2.12171\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 790\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 2.01239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.79174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.79076\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▅▄▃▃▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▃▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer █▂▃▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 179 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbreezy-glitter-1\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/1lbo72mk\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2f2zuxUaJzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba11cbb-2b87-442c-ffe9-4bf4308fec1f"
      },
      "source": [
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-m86ur3wn\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-m86ur3wn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (0.8.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (0.18.2)\n",
            "Collecting PyYAML!=5.4.*,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.7.4.post0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.3.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.3.0.dev0) (20.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.3.0.dev0) (0.4.8)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.3.0.dev0-cp37-none-any.whl size=829805 sha256=767676c1cff1d76165fa3c2216176d27a9bce502f1a61263d18c4178cc05510f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-57sx947m/wheels/e2/c6/88/caa5d4cfbfab631fc84b0107896a6f661a1caf589160c27e71\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=0e2e213c5402222795d3ac921015a86c75d2f67c845f4556662098e84528c0bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML, pytorch-lightning\n",
            "  Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Found existing installation: pytorch-lightning 1.1.4\n",
            "    Uninstalling pytorch-lightning-1.1.4:\n",
            "      Successfully uninstalled pytorch-lightning-1.1.4\n",
            "Successfully installed PyYAML-5.3.1 pytorch-lightning-1.3.0.dev0\n",
            "1.3.0dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSohD0bedq4F",
        "outputId": "66a068c9-1ebb-4019-b766-e5937f26e6ab"
      },
      "source": [
        "!wandb sweep training/sweeps/emnist_lines2_line_cnn_transformer.yml"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: training/sweeps/emnist_lines2_line_cnn_transformer.yml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33mgt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent lucas-spangher/fsdl_lab_5/gt2t31i7\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZNcu4OsigLK",
        "outputId": "3786d504-75d5-4da0-fae3-dea3388f7bf9"
      },
      "source": [
        "!wandb agent lucas-spangher/fsdl_lab_5/gt2t31i7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2021-03-10 00:30:12,917 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-03-10 00:30:13,097 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 00:30:13,097 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.001\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 4\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 00:30:13,099 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.001 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=4 --tf_nhead=8 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 00:30:15.614888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/z62dg7ms\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 00:30:18,111 - wandb.wandb_agent - INFO - Running runs: ['z62dg7ms']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 2.7 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.1 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 764 K \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 459 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 459 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33  | model.embedding                                            | Embedding               | 10.6 K\n",
            "34  | model.fc                                                   | Linear                  | 10.7 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 1.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 1.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 396 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 66.0 K\n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 132 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 131 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 256   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 256   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 256   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 396 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 66.0 K\n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 132 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 131 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 256   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 256   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 256   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "2.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.7 M     Total params\n",
            "10.794    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=2.74, v_num=g7ms, val_loss=4.590, val_cer=0.979]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:59<00:11,  1.33it/s, loss=2.74, v_num=g7ms, val_loss=4.590, val_cer=0.979]\n",
            "Epoch 0: 100%|██████████| 95/95 [01:25<00:00,  1.11it/s, loss=2.6, v_num=g7ms, val_loss=2.530, val_cer=0.811] \n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=2.48, v_num=g7ms, val_loss=2.530, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=2.48, v_num=g7ms, val_loss=2.530, val_cer=0.811]\n",
            "Epoch 1: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.45, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=2.38, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=2.38, v_num=g7ms, val_loss=2.370, val_cer=0.821]\n",
            "Epoch 2: 100%|██████████| 95/95 [01:27<00:00,  1.08it/s, loss=2.36, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=2.32, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=2.32, v_num=g7ms, val_loss=2.280, val_cer=0.821]\n",
            "Epoch 3: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.29, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=2.26, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 80/95 [01:02<00:11,  1.29it/s, loss=2.26, v_num=g7ms, val_loss=2.200, val_cer=0.821]\n",
            "Epoch 4: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.25, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=2.21, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 80/95 [01:04<00:12,  1.23it/s, loss=2.21, v_num=g7ms, val_loss=2.150, val_cer=0.797]\n",
            "Epoch 5: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=2.21, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=2.19, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=2.19, v_num=g7ms, val_loss=2.120, val_cer=0.810]\n",
            "Epoch 6: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.18, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=2.17, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:00<00:11,  1.32it/s, loss=2.17, v_num=g7ms, val_loss=2.080, val_cer=0.798]\n",
            "Epoch 7: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.16, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=2.14, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.14, v_num=g7ms, val_loss=2.060, val_cer=0.800]\n",
            "Epoch 8: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=2.14, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=2.12, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|████████▍ | 80/95 [01:06<00:12,  1.21it/s, loss=2.12, v_num=g7ms, val_loss=2.040, val_cer=0.794]\n",
            "Epoch 9: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Epoch 10:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=2.11, v_num=g7ms, val_loss=2.020, val_cer=0.805]\n",
            "Epoch 10: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=2.11, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Epoch 11:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=2.08, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=2.08, v_num=g7ms, val_loss=2.000, val_cer=0.792]\n",
            "Epoch 11: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.09, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Epoch 12:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=2.08, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|████████▍ | 80/95 [01:05<00:12,  1.23it/s, loss=2.08, v_num=g7ms, val_loss=1.990, val_cer=0.800]\n",
            "Epoch 12: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:58<00:10,  1.38it/s, loss=2.07, v_num=g7ms, val_loss=1.980, val_cer=0.807]\n",
            "Epoch 13: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=2.05, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Epoch 14:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=2.04, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|████████▍ | 80/95 [01:01<00:11,  1.30it/s, loss=2.04, v_num=g7ms, val_loss=1.960, val_cer=0.792]\n",
            "Epoch 14: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=2.06, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Epoch 15:  84%|████████▍ | 80/95 [00:47<00:08,  1.69it/s, loss=2.04, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|████████▍ | 80/95 [01:04<00:12,  1.24it/s, loss=2.04, v_num=g7ms, val_loss=1.950, val_cer=0.792]\n",
            "Epoch 15: 100%|██████████| 95/95 [01:28<00:00,  1.08it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Epoch 16:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|████████▍ | 80/95 [01:05<00:12,  1.21it/s, loss=2.03, v_num=g7ms, val_loss=1.940, val_cer=0.792]\n",
            "Epoch 16: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=2.04, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Epoch 17:  84%|████████▍ | 80/95 [00:46<00:08,  1.70it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|████████▍ | 80/95 [00:58<00:10,  1.37it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.786]\n",
            "Epoch 17: 100%|██████████| 95/95 [01:27<00:00,  1.08it/s, loss=2.02, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Epoch 18:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=2.01, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=2.01, v_num=g7ms, val_loss=1.920, val_cer=0.793]\n",
            "Epoch 18: 100%|██████████| 95/95 [01:28<00:00,  1.08it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]   \n",
            "Epoch 19:  84%|████████▍ | 80/95 [00:46<00:08,  1.70it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|████████▍ | 80/95 [01:02<00:11,  1.29it/s, loss=2, v_num=g7ms, val_loss=1.910, val_cer=0.800]\n",
            "Epoch 19: 100%|██████████| 95/95 [01:28<00:00,  1.08it/s, loss=2.01, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Epoch 20:  84%|████████▍ | 80/95 [00:47<00:08,  1.70it/s, loss=1.99, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.99, v_num=g7ms, val_loss=1.910, val_cer=0.793]\n",
            "Epoch 20: 100%|██████████| 95/95 [01:27<00:00,  1.08it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Epoch 21:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|████████▍ | 80/95 [01:05<00:12,  1.22it/s, loss=1.99, v_num=g7ms, val_loss=1.900, val_cer=0.808]\n",
            "Epoch 21: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Epoch 22:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 22:  84%|████████▍ | 80/95 [00:58<00:10,  1.36it/s, loss=1.99, v_num=g7ms, val_loss=1.890, val_cer=0.801]\n",
            "Epoch 22: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 23:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 23:  84%|████████▍ | 80/95 [01:01<00:11,  1.30it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 23: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=1.97, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 24:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.95, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 24:  84%|████████▍ | 80/95 [01:04<00:12,  1.24it/s, loss=1.95, v_num=g7ms, val_loss=1.880, val_cer=0.792]\n",
            "Epoch 24: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.98, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Epoch 25:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.96, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 25:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=1.96, v_num=g7ms, val_loss=1.870, val_cer=0.795]\n",
            "Epoch 25: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Epoch 26:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 26:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.95, v_num=g7ms, val_loss=1.870, val_cer=0.799]\n",
            "Epoch 26: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 27:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.96, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 27:  84%|████████▍ | 80/95 [01:04<00:12,  1.24it/s, loss=1.96, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 27: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Epoch 28:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 28:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=1.94, v_num=g7ms, val_loss=1.860, val_cer=0.808]\n",
            "Epoch 28: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 29:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 29:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.95, v_num=g7ms, val_loss=1.860, val_cer=0.799]\n",
            "Epoch 29: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 30:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 30:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 30: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.93, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 31:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.92, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 31:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=1.92, v_num=g7ms, val_loss=1.850, val_cer=0.799]\n",
            "Epoch 31: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.93, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Epoch 32:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 32:  84%|████████▍ | 80/95 [00:59<00:11,  1.33it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.799]\n",
            "Epoch 32: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Epoch 33:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.91, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 33:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=1.91, v_num=g7ms, val_loss=1.840, val_cer=0.792]\n",
            "Epoch 33: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Epoch 34:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.9, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 34:  84%|████████▍ | 80/95 [00:57<00:10,  1.40it/s, loss=1.9, v_num=g7ms, val_loss=1.840, val_cer=0.789]\n",
            "Epoch 34: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Epoch 35:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 35:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.792]\n",
            "Epoch 35: 100%|██████████| 95/95 [01:25<00:00,  1.11it/s, loss=1.92, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Epoch 36:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.9, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 36:  84%|████████▍ | 80/95 [01:05<00:12,  1.23it/s, loss=1.9, v_num=g7ms, val_loss=1.830, val_cer=0.795]\n",
            "Epoch 36: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.92, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Epoch 37:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 37:  84%|████████▍ | 80/95 [00:58<00:11,  1.36it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.797]\n",
            "Epoch 37: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.9, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 38:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 38:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 38: 100%|██████████| 95/95 [01:25<00:00,  1.11it/s, loss=1.9, v_num=g7ms, val_loss=1.810, val_cer=0.792] \n",
            "Epoch 39:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.792]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  84%|████████▍ | 80/95 [00:56<00:10,  1.42it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.792]\n",
            "Epoch 39: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.91, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 40:  84%|████████▍ | 80/95 [00:45<00:08,  1.77it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 40:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 40: 100%|██████████| 95/95 [01:25<00:00,  1.11it/s, loss=1.89, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 41:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 41:  84%|████████▍ | 80/95 [01:04<00:12,  1.24it/s, loss=1.88, v_num=g7ms, val_loss=1.820, val_cer=0.799]\n",
            "Epoch 41: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 42:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 42:  84%|████████▍ | 80/95 [00:57<00:10,  1.38it/s, loss=1.89, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 42: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 43:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 43:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.88, v_num=g7ms, val_loss=1.810, val_cer=0.795]\n",
            "Epoch 43: 100%|██████████| 95/95 [01:26<00:00,  1.09it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Epoch 44:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 44:  84%|████████▍ | 80/95 [01:04<00:12,  1.25it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.799]\n",
            "Epoch 44: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 45:  84%|████████▍ | 80/95 [00:45<00:08,  1.76it/s, loss=1.89, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 45:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=1.89, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 45: 100%|██████████| 95/95 [01:25<00:00,  1.11it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 46:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 46:  84%|████████▍ | 80/95 [01:01<00:11,  1.30it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 46: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 47:  84%|████████▍ | 80/95 [00:45<00:08,  1.77it/s, loss=1.87, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 47:  84%|████████▍ | 80/95 [00:55<00:10,  1.44it/s, loss=1.87, v_num=g7ms, val_loss=1.800, val_cer=0.795]\n",
            "Epoch 47: 100%|██████████| 95/95 [01:25<00:00,  1.10it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 48:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 48:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=1.87, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 48: 100%|██████████| 95/95 [01:26<00:00,  1.10it/s, loss=1.88, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 49:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=1.86, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 49:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.86, v_num=g7ms, val_loss=1.790, val_cer=0.795]\n",
            "Epoch 49: 100%|██████████| 95/95 [01:27<00:00,  1.09it/s, loss=1.86, v_num=g7ms, val_loss=1.780, val_cer=0.795]\n",
            "Epoch 49: 100%|██████████| 95/95 [01:27<00:00,  1.08it/s, loss=1.86, v_num=g7ms, val_loss=1.780, val_cer=0.795]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:33<00:00,  2.12s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.7965301275253296}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_003014-z62dg7ms/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 4396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615340610\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 1.75214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 1.78405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.79481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.79653\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ▆███▆▃▄▃▂▄▅▂▂▁▂▄▅▄▂▂▄▄▅▄▄▄▂▂▃▃▄▂▄▃▃▄▃▃▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcool-sweep-1\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/z62dg7ms\u001b[0m\n",
            "2021-03-10 01:43:38,863 - wandb.wandb_agent - INFO - Cleaning up finished run: z62dg7ms\n",
            "2021-03-10 01:43:40,748 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 01:43:40,748 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 2\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 01:43:40,750 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=2 --tf_nhead=8 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 01:43:43.183716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdifferent-sweep-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/n5egke13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 01:43:45,759 - wandb.wandb_agent - INFO - Running runs: ['n5egke13']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name                                                       | Type                    | Params\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "0  | model                                                      | LineCNNTransformer      | 4.2 M \n",
            "1  | model.line_cnn                                             | LineCNN                 | 3.4 M \n",
            "2  | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3  | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4  | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5  | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6  | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7  | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8  | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9  | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10 | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11 | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12 | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13 | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14 | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15 | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16 | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17 | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18 | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19 | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20 | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21 | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22 | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23 | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24 | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25 | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26 | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27 | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28 | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29 | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30 | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31 | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32 | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33 | model.embedding                                            | Embedding               | 10.6 K\n",
            "34 | model.fc                                                   | Linear                  | 10.7 K\n",
            "35 | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36 | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37 | model.transformer_decoder                                  | TransformerDecoder      | 792 K \n",
            "38 | model.transformer_decoder.layers                           | ModuleList              | 792 K \n",
            "39 | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40 | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41 | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42 | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43 | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44 | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45 | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46 | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47 | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48 | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49 | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50 | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51 | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52 | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53 | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54 | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55 | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56 | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57 | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58 | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59 | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60 | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61 | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62 | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63 | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64 | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65 | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66 | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67 | train_acc                                                  | Accuracy                | 0     \n",
            "68 | val_acc                                                    | Accuracy                | 0     \n",
            "69 | test_acc                                                   | Accuracy                | 0     \n",
            "70 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "71 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "72 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "4.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.2 M     Total params\n",
            "16.782    Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=2.87, v_num=ke13, val_loss=4.800, val_cer=0.990]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 95/95 [01:35<00:00,  1.01s/it, loss=2.74, v_num=ke13, val_loss=2.640, val_cer=0.811]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [01:01<00:11,  1.29it/s, loss=2.6, v_num=ke13, val_loss=2.640, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 95/95 [01:36<00:00,  1.01s/it, loss=2.57, v_num=ke13, val_loss=2.480, val_cer=0.766]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=2.49, v_num=ke13, val_loss=2.480, val_cer=0.766]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 95/95 [01:37<00:00,  1.02s/it, loss=2.48, v_num=ke13, val_loss=2.400, val_cer=0.770]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [01:02<00:11,  1.29it/s, loss=2.44, v_num=ke13, val_loss=2.400, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 95/95 [01:36<00:00,  1.02s/it, loss=2.44, v_num=ke13, val_loss=2.350, val_cer=0.770]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=2.4, v_num=ke13, val_loss=2.350, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 95/95 [01:36<00:00,  1.02s/it, loss=2.39, v_num=ke13, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.35, v_num=ke13, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.35, v_num=ke13, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=2.32, v_num=ke13, val_loss=2.240, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.32, v_num=ke13, val_loss=2.230, val_cer=0.772]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=2.28, v_num=ke13, val_loss=2.230, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.28, v_num=ke13, val_loss=2.170, val_cer=0.774]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.25, v_num=ke13, val_loss=2.170, val_cer=0.774]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=2.26, v_num=ke13, val_loss=2.160, val_cer=0.771]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=2.23, v_num=ke13, val_loss=2.160, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.23, v_num=ke13, val_loss=2.120, val_cer=0.770]\n",
            "Epoch 10:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.22, v_num=ke13, val_loss=2.120, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=2.2, v_num=ke13, val_loss=2.080, val_cer=0.768] \n",
            "Epoch 11:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.17, v_num=ke13, val_loss=2.080, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=2.18, v_num=ke13, val_loss=2.060, val_cer=0.768]\n",
            "Epoch 12:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=2.16, v_num=ke13, val_loss=2.060, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.16, v_num=ke13, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=2.13, v_num=ke13, val_loss=2.030, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.13, v_num=ke13, val_loss=1.990, val_cer=0.760]\n",
            "Epoch 14:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=2.1, v_num=ke13, val_loss=1.990, val_cer=0.760]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=2.1, v_num=ke13, val_loss=1.980, val_cer=0.759]\n",
            "Epoch 15:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=2.08, v_num=ke13, val_loss=1.980, val_cer=0.759]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.08, v_num=ke13, val_loss=1.960, val_cer=0.749]\n",
            "Epoch 16:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=2.04, v_num=ke13, val_loss=1.960, val_cer=0.749]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=2.02, v_num=ke13, val_loss=1.900, val_cer=0.734]\n",
            "Epoch 17:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.01, v_num=ke13, val_loss=1.900, val_cer=0.734]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=2, v_num=ke13, val_loss=1.860, val_cer=0.724]   \n",
            "Epoch 18:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.98, v_num=ke13, val_loss=1.860, val_cer=0.724]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.96, v_num=ke13, val_loss=1.790, val_cer=0.711]\n",
            "Epoch 19:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=1.92, v_num=ke13, val_loss=1.790, val_cer=0.711]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=1.92, v_num=ke13, val_loss=1.720, val_cer=0.686]\n",
            "Epoch 20:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=1.86, v_num=ke13, val_loss=1.720, val_cer=0.686]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=1.85, v_num=ke13, val_loss=1.650, val_cer=0.658]\n",
            "Epoch 21:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.8, v_num=ke13, val_loss=1.650, val_cer=0.658]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.79, v_num=ke13, val_loss=1.580, val_cer=0.650]\n",
            "Epoch 22:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=1.73, v_num=ke13, val_loss=1.580, val_cer=0.650]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=1.73, v_num=ke13, val_loss=1.470, val_cer=0.601]\n",
            "Epoch 23:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.64, v_num=ke13, val_loss=1.470, val_cer=0.601]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=1.64, v_num=ke13, val_loss=1.380, val_cer=0.567]\n",
            "Epoch 24:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.59, v_num=ke13, val_loss=1.380, val_cer=0.567]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.57, v_num=ke13, val_loss=1.270, val_cer=0.538]\n",
            "Epoch 25:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.51, v_num=ke13, val_loss=1.270, val_cer=0.538]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.5, v_num=ke13, val_loss=1.200, val_cer=0.508] \n",
            "Epoch 26:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.42, v_num=ke13, val_loss=1.200, val_cer=0.508]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.41, v_num=ke13, val_loss=1.090, val_cer=0.462]\n",
            "Epoch 27:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.36, v_num=ke13, val_loss=1.090, val_cer=0.462]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.33, v_num=ke13, val_loss=1.000, val_cer=0.434]\n",
            "Epoch 28:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=1.29, v_num=ke13, val_loss=1.000, val_cer=0.434]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.26, v_num=ke13, val_loss=0.926, val_cer=0.402]\n",
            "Epoch 29:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=1.22, v_num=ke13, val_loss=0.926, val_cer=0.402]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.2, v_num=ke13, val_loss=0.849, val_cer=0.372] \n",
            "Epoch 30:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=1.17, v_num=ke13, val_loss=0.849, val_cer=0.372]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=1.15, v_num=ke13, val_loss=0.800, val_cer=0.352]\n",
            "Epoch 31:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.1, v_num=ke13, val_loss=0.800, val_cer=0.352]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=1.11, v_num=ke13, val_loss=0.778, val_cer=0.344]\n",
            "Epoch 32:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=1.05, v_num=ke13, val_loss=0.778, val_cer=0.344]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=1.04, v_num=ke13, val_loss=0.726, val_cer=0.311]\n",
            "Epoch 33:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=1, v_num=ke13, val_loss=0.726, val_cer=0.311]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33: 100%|██████████| 95/95 [01:39<00:00,  1.04s/it, loss=1.01, v_num=ke13, val_loss=0.678, val_cer=0.294]\n",
            "Epoch 34:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.967, v_num=ke13, val_loss=0.678, val_cer=0.294]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=0.956, v_num=ke13, val_loss=0.650, val_cer=0.279]\n",
            "Epoch 35:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.94, v_num=ke13, val_loss=0.650, val_cer=0.279]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.917, v_num=ke13, val_loss=0.605, val_cer=0.252]\n",
            "Epoch 36:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.899, v_num=ke13, val_loss=0.605, val_cer=0.252]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=0.904, v_num=ke13, val_loss=0.604, val_cer=0.259]\n",
            "Epoch 37:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=0.86, v_num=ke13, val_loss=0.604, val_cer=0.259]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.846, v_num=ke13, val_loss=0.556, val_cer=0.239]\n",
            "Epoch 38:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=0.82, v_num=ke13, val_loss=0.556, val_cer=0.239]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38: 100%|██████████| 95/95 [01:37<00:00,  1.02s/it, loss=0.805, v_num=ke13, val_loss=0.560, val_cer=0.227]\n",
            "Epoch 39:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.823, v_num=ke13, val_loss=0.560, val_cer=0.227]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.799, v_num=ke13, val_loss=0.513, val_cer=0.219]\n",
            "Epoch 40:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=0.778, v_num=ke13, val_loss=0.513, val_cer=0.219]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=0.785, v_num=ke13, val_loss=0.525, val_cer=0.219]\n",
            "Epoch 41:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=0.746, v_num=ke13, val_loss=0.525, val_cer=0.219]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=0.738, v_num=ke13, val_loss=0.461, val_cer=0.197]\n",
            "Epoch 42:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.735, v_num=ke13, val_loss=0.461, val_cer=0.197]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.726, v_num=ke13, val_loss=0.473, val_cer=0.203]\n",
            "Epoch 43:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.734, v_num=ke13, val_loss=0.473, val_cer=0.203]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=0.716, v_num=ke13, val_loss=0.435, val_cer=0.179]\n",
            "Epoch 44:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=0.692, v_num=ke13, val_loss=0.435, val_cer=0.179]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.692, v_num=ke13, val_loss=0.419, val_cer=0.173]\n",
            "Epoch 45:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=0.68, v_num=ke13, val_loss=0.419, val_cer=0.173]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=0.658, v_num=ke13, val_loss=0.407, val_cer=0.169]\n",
            "Epoch 46:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=0.662, v_num=ke13, val_loss=0.407, val_cer=0.169]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46: 100%|██████████| 95/95 [01:38<00:00,  1.03s/it, loss=0.635, v_num=ke13, val_loss=0.412, val_cer=0.170]\n",
            "Epoch 47:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=0.631, v_num=ke13, val_loss=0.412, val_cer=0.170]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=0.628, v_num=ke13, val_loss=0.392, val_cer=0.159]\n",
            "Epoch 48:  84%|████████▍ | 80/95 [01:03<00:11,  1.25it/s, loss=0.63, v_num=ke13, val_loss=0.392, val_cer=0.159]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48: 100%|██████████| 95/95 [01:38<00:00,  1.04s/it, loss=0.633, v_num=ke13, val_loss=0.431, val_cer=0.167]\n",
            "Epoch 49:  84%|████████▍ | 80/95 [01:03<00:11,  1.27it/s, loss=0.625, v_num=ke13, val_loss=0.431, val_cer=0.167]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=0.612, v_num=ke13, val_loss=0.367, val_cer=0.150]\n",
            "Epoch 49: 100%|██████████| 95/95 [01:37<00:00,  1.03s/it, loss=0.612, v_num=ke13, val_loss=0.367, val_cer=0.150]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:23<00:00,  1.49s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.1210828348994255}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 9821\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_014342-n5egke13/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 4945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615345567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.60932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.36696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.14975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.12108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ███████████▇▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdifferent-sweep-2\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/n5egke13\u001b[0m\n",
            "2021-03-10 03:06:15,923 - wandb.wandb_agent - INFO - Cleaning up finished run: n5egke13\n",
            "2021-03-10 03:06:16,134 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 03:06:16,135 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 2\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 03:06:16,137 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=2 --tf_nhead=4 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 03:06:18.463692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-sweep-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/48whs9dq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 03:06:21,145 - wandb.wandb_agent - INFO - Running runs: ['48whs9dq']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name                                                       | Type                    | Params\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "0  | model                                                      | LineCNNTransformer      | 3.2 M \n",
            "1  | model.line_cnn                                             | LineCNN                 | 2.4 M \n",
            "2  | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3  | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4  | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5  | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6  | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7  | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8  | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9  | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10 | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11 | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12 | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13 | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14 | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15 | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16 | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17 | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18 | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19 | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20 | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21 | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22 | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23 | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24 | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25 | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26 | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27 | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28 | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29 | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30 | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31 | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32 | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33 | model.embedding                                            | Embedding               | 10.6 K\n",
            "34 | model.fc                                                   | Linear                  | 10.7 K\n",
            "35 | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36 | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37 | model.transformer_decoder                                  | TransformerDecoder      | 792 K \n",
            "38 | model.transformer_decoder.layers                           | ModuleList              | 792 K \n",
            "39 | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40 | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41 | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42 | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43 | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44 | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45 | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46 | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47 | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48 | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49 | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50 | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51 | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52 | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53 | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54 | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55 | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56 | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57 | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58 | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59 | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60 | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61 | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62 | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63 | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64 | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65 | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66 | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67 | train_acc                                                  | Accuracy                | 0     \n",
            "68 | val_acc                                                    | Accuracy                | 0     \n",
            "69 | test_acc                                                   | Accuracy                | 0     \n",
            "70 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "71 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "72 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.872    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:43<00:08,  1.83it/s, loss=2.84, v_num=s9dq, val_loss=4.510, val_cer=0.980]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:59<00:11,  1.33it/s, loss=2.84, v_num=s9dq, val_loss=4.510, val_cer=0.980]\n",
            "Epoch 0: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.72, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.57, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=2.57, v_num=s9dq, val_loss=2.600, val_cer=0.790]\n",
            "Epoch 1: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.54, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.48, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:55<00:10,  1.43it/s, loss=2.48, v_num=s9dq, val_loss=2.460, val_cer=0.770]\n",
            "Epoch 2: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.46, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.43, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:58<00:11,  1.36it/s, loss=2.43, v_num=s9dq, val_loss=2.420, val_cer=0.773]\n",
            "Epoch 3: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.43, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.4, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 80/95 [01:01<00:11,  1.30it/s, loss=2.4, v_num=s9dq, val_loss=2.330, val_cer=0.772]\n",
            "Epoch 4: 100%|██████████| 95/95 [01:07<00:00,  1.41it/s, loss=2.39, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.36, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 80/95 [00:54<00:10,  1.47it/s, loss=2.36, v_num=s9dq, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.34, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.31, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:57<00:10,  1.40it/s, loss=2.31, v_num=s9dq, val_loss=2.240, val_cer=0.772]\n",
            "Epoch 6: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.31, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.3, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=2.3, v_num=s9dq, val_loss=2.210, val_cer=0.773]\n",
            "Epoch 7: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.29, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.26, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=2.26, v_num=s9dq, val_loss=2.170, val_cer=0.772]\n",
            "Epoch 8: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.25, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|████████▍ | 80/95 [00:56<00:10,  1.42it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.769]\n",
            "Epoch 9: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=2.24, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Epoch 10:  84%|████████▍ | 80/95 [00:43<00:08,  1.83it/s, loss=2.22, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=2.22, v_num=s9dq, val_loss=2.150, val_cer=0.758]\n",
            "Epoch 10: 100%|██████████| 95/95 [01:07<00:00,  1.40it/s, loss=2.2, v_num=s9dq, val_loss=2.090, val_cer=0.768] \n",
            "Epoch 11:  84%|████████▍ | 80/95 [00:44<00:08,  1.81it/s, loss=2.18, v_num=s9dq, val_loss=2.090, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|████████▍ | 80/95 [01:01<00:11,  1.30it/s, loss=2.18, v_num=s9dq, val_loss=2.090, val_cer=0.768]\n",
            "Epoch 11: 100%|██████████| 95/95 [01:08<00:00,  1.39it/s, loss=2.19, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Epoch 12:  84%|████████▍ | 80/95 [00:44<00:08,  1.82it/s, loss=2.16, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=2.16, v_num=s9dq, val_loss=2.060, val_cer=0.771]\n",
            "Epoch 12: 100%|██████████| 95/95 [01:08<00:00,  1.39it/s, loss=2.16, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:44<00:08,  1.81it/s, loss=2.13, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:54<00:10,  1.47it/s, loss=2.13, v_num=s9dq, val_loss=2.030, val_cer=0.767]\n",
            "Epoch 13: 100%|██████████| 95/95 [01:08<00:00,  1.39it/s, loss=2.12, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Epoch 14:  84%|████████▍ | 80/95 [00:44<00:08,  1.82it/s, loss=2.11, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|████████▍ | 80/95 [00:56<00:10,  1.42it/s, loss=2.11, v_num=s9dq, val_loss=2.010, val_cer=0.763]\n",
            "Epoch 14: 100%|██████████| 95/95 [01:08<00:00,  1.40it/s, loss=2.12, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Epoch 15:  84%|████████▍ | 80/95 [00:44<00:08,  1.80it/s, loss=2.09, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|████████▍ | 80/95 [00:57<00:10,  1.38it/s, loss=2.09, v_num=s9dq, val_loss=2.000, val_cer=0.759]\n",
            "Epoch 15: 100%|██████████| 95/95 [01:08<00:00,  1.39it/s, loss=2.08, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Epoch 16:  84%|████████▍ | 80/95 [00:44<00:08,  1.82it/s, loss=2.07, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=2.07, v_num=s9dq, val_loss=1.950, val_cer=0.758]\n",
            "Epoch 16: 100%|██████████| 95/95 [01:08<00:00,  1.40it/s, loss=2.05, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Epoch 17:  84%|████████▍ | 80/95 [00:43<00:08,  1.86it/s, loss=2.04, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=2.04, v_num=s9dq, val_loss=1.900, val_cer=0.735]\n",
            "Epoch 17: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=2.02, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Epoch 18:  84%|████████▍ | 80/95 [00:42<00:08,  1.86it/s, loss=1.99, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|████████▍ | 80/95 [00:54<00:10,  1.47it/s, loss=1.99, v_num=s9dq, val_loss=1.860, val_cer=0.729]\n",
            "Epoch 18: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=1.99, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Epoch 19:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.95, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|████████▍ | 80/95 [00:58<00:10,  1.38it/s, loss=1.95, v_num=s9dq, val_loss=1.800, val_cer=0.716]\n",
            "Epoch 19: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.95, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Epoch 20:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.91, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.91, v_num=s9dq, val_loss=1.750, val_cer=0.701]\n",
            "Epoch 20: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.9, v_num=s9dq, val_loss=1.690, val_cer=0.681] \n",
            "Epoch 21:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.86, v_num=s9dq, val_loss=1.690, val_cer=0.681]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|████████▍ | 80/95 [00:54<00:10,  1.47it/s, loss=1.86, v_num=s9dq, val_loss=1.690, val_cer=0.681]\n",
            "Epoch 21: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.85, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Epoch 22:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=1.81, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 22:  84%|████████▍ | 80/95 [00:57<00:10,  1.40it/s, loss=1.81, v_num=s9dq, val_loss=1.640, val_cer=0.669]\n",
            "Epoch 22: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.81, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Epoch 23:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.76, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 23:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=1.76, v_num=s9dq, val_loss=1.590, val_cer=0.652]\n",
            "Epoch 23: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=1.76, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Epoch 24:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.7, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 24:  84%|████████▍ | 80/95 [00:53<00:10,  1.49it/s, loss=1.7, v_num=s9dq, val_loss=1.500, val_cer=0.609]\n",
            "Epoch 24: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=1.69, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Epoch 25:  84%|████████▍ | 80/95 [00:43<00:08,  1.86it/s, loss=1.67, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 25:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=1.67, v_num=s9dq, val_loss=1.410, val_cer=0.585]\n",
            "Epoch 25: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=1.65, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Epoch 26:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=1.59, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 26:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=1.59, v_num=s9dq, val_loss=1.350, val_cer=0.568]\n",
            "Epoch 26: 100%|██████████| 95/95 [01:07<00:00,  1.41it/s, loss=1.6, v_num=s9dq, val_loss=1.270, val_cer=0.536] \n",
            "Epoch 27:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.54, v_num=s9dq, val_loss=1.270, val_cer=0.536]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 27:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=1.54, v_num=s9dq, val_loss=1.270, val_cer=0.536]\n",
            "Epoch 27: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.54, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Epoch 28:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.48, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 28:  84%|████████▍ | 80/95 [00:55<00:10,  1.43it/s, loss=1.48, v_num=s9dq, val_loss=1.240, val_cer=0.515]\n",
            "Epoch 28: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=1.46, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Epoch 29:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.45, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 29:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=1.45, v_num=s9dq, val_loss=1.200, val_cer=0.517]\n",
            "Epoch 29: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.44, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Epoch 30:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.39, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 30:  84%|████████▍ | 80/95 [01:02<00:11,  1.29it/s, loss=1.39, v_num=s9dq, val_loss=1.120, val_cer=0.480]\n",
            "Epoch 30: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.38, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Epoch 31:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=1.35, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 31:  84%|████████▍ | 80/95 [00:55<00:10,  1.45it/s, loss=1.35, v_num=s9dq, val_loss=1.050, val_cer=0.463]\n",
            "Epoch 31: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.33, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Epoch 32:  84%|████████▍ | 80/95 [00:43<00:08,  1.83it/s, loss=1.32, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 32:  84%|████████▍ | 80/95 [00:58<00:10,  1.37it/s, loss=1.32, v_num=s9dq, val_loss=1.100, val_cer=0.472]\n",
            "Epoch 32: 100%|██████████| 95/95 [01:07<00:00,  1.41it/s, loss=1.31, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Epoch 33:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=1.28, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 33:  84%|████████▍ | 80/95 [01:01<00:11,  1.31it/s, loss=1.28, v_num=s9dq, val_loss=0.973, val_cer=0.427]\n",
            "Epoch 33: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.26, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Epoch 34:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=1.24, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 34:  84%|████████▍ | 80/95 [00:53<00:10,  1.48it/s, loss=1.24, v_num=s9dq, val_loss=0.924, val_cer=0.404]\n",
            "Epoch 34: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.23, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Epoch 35:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.2, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 35:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=1.2, v_num=s9dq, val_loss=0.894, val_cer=0.393]\n",
            "Epoch 35: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.22, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Epoch 36:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.16, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 36:  84%|████████▍ | 80/95 [01:00<00:11,  1.33it/s, loss=1.16, v_num=s9dq, val_loss=0.829, val_cer=0.362]\n",
            "Epoch 36: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.16, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Epoch 37:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.15, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 37:  84%|████████▍ | 80/95 [00:53<00:10,  1.50it/s, loss=1.15, v_num=s9dq, val_loss=0.832, val_cer=0.370]\n",
            "Epoch 37: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.13, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Epoch 38:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=1.09, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 38:  84%|████████▍ | 80/95 [00:56<00:10,  1.42it/s, loss=1.09, v_num=s9dq, val_loss=0.791, val_cer=0.347]\n",
            "Epoch 38: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=1.09, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Epoch 39:  84%|████████▍ | 80/95 [00:43<00:08,  1.83it/s, loss=1.07, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  84%|████████▍ | 80/95 [00:59<00:11,  1.34it/s, loss=1.07, v_num=s9dq, val_loss=0.748, val_cer=0.320]\n",
            "Epoch 39: 100%|██████████| 95/95 [01:07<00:00,  1.41it/s, loss=1.06, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Epoch 40:  84%|████████▍ | 80/95 [00:43<00:08,  1.82it/s, loss=1.04, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 40:  84%|████████▍ | 80/95 [01:02<00:11,  1.29it/s, loss=1.04, v_num=s9dq, val_loss=0.723, val_cer=0.320]\n",
            "Epoch 40: 100%|██████████| 95/95 [01:07<00:00,  1.40it/s, loss=1.03, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Epoch 41:  84%|████████▍ | 80/95 [00:43<00:08,  1.82it/s, loss=1.04, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 41:  84%|████████▍ | 80/95 [00:54<00:10,  1.47it/s, loss=1.04, v_num=s9dq, val_loss=0.705, val_cer=0.301]\n",
            "Epoch 41: 100%|██████████| 95/95 [01:07<00:00,  1.41it/s, loss=1, v_num=s9dq, val_loss=0.700, val_cer=0.297]   \n",
            "Epoch 42:  84%|████████▍ | 80/95 [00:43<00:08,  1.83it/s, loss=1.01, v_num=s9dq, val_loss=0.700, val_cer=0.297]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 42:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=1.01, v_num=s9dq, val_loss=0.700, val_cer=0.297]\n",
            "Epoch 42: 100%|██████████| 95/95 [01:07<00:00,  1.42it/s, loss=1, v_num=s9dq, val_loss=0.665, val_cer=0.289]   \n",
            "Epoch 43:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=0.994, v_num=s9dq, val_loss=0.665, val_cer=0.289]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 43:  84%|████████▍ | 80/95 [00:59<00:11,  1.34it/s, loss=0.994, v_num=s9dq, val_loss=0.665, val_cer=0.289]\n",
            "Epoch 43: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=0.986, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Epoch 44:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=0.961, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 44:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=0.961, v_num=s9dq, val_loss=0.724, val_cer=0.302]\n",
            "Epoch 44: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=0.947, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Epoch 45:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=0.945, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 45:  84%|████████▍ | 80/95 [00:55<00:10,  1.43it/s, loss=0.945, v_num=s9dq, val_loss=0.624, val_cer=0.270]\n",
            "Epoch 45: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=0.928, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Epoch 46:  84%|████████▍ | 80/95 [00:42<00:08,  1.86it/s, loss=0.913, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 46:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=0.913, v_num=s9dq, val_loss=0.612, val_cer=0.258]\n",
            "Epoch 46: 100%|██████████| 95/95 [01:06<00:00,  1.43it/s, loss=0.916, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Epoch 47:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=0.912, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 47:  84%|████████▍ | 80/95 [01:02<00:11,  1.28it/s, loss=0.912, v_num=s9dq, val_loss=0.593, val_cer=0.267]\n",
            "Epoch 47: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=0.901, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Epoch 48:  84%|████████▍ | 80/95 [00:43<00:08,  1.84it/s, loss=0.878, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 48:  84%|████████▍ | 80/95 [00:55<00:10,  1.43it/s, loss=0.878, v_num=s9dq, val_loss=0.596, val_cer=0.250]\n",
            "Epoch 48: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=0.889, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Epoch 49:  84%|████████▍ | 80/95 [00:43<00:08,  1.85it/s, loss=0.869, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 49:  84%|████████▍ | 80/95 [00:58<00:11,  1.36it/s, loss=0.869, v_num=s9dq, val_loss=0.562, val_cer=0.242]\n",
            "Epoch 49: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=0.878, v_num=s9dq, val_loss=0.555, val_cer=0.245]\n",
            "Epoch 49: 100%|██████████| 95/95 [01:06<00:00,  1.42it/s, loss=0.878, v_num=s9dq, val_loss=0.555, val_cer=0.245]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.1871183067560196}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 18664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_030617-48whs9dq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 3391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615348968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.83147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.55534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.2448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.18712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer █████████████▇▇▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfancy-sweep-3\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/48whs9dq\u001b[0m\n",
            "2021-03-10 04:02:55,114 - wandb.wandb_agent - INFO - Cleaning up finished run: 48whs9dq\n",
            "2021-03-10 04:02:57,333 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 04:02:57,333 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 512\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.0003\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 128\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 6\n",
            "\ttf_nhead: 8\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 04:02:57,335 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=512 --gpus=-1 --loss=transformer --lr=0.0003 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=128 --tf_fc_dim=1024 --tf_layers=6 --tf_nhead=8 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 04:02:59.701015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-sweep-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/v3bw7ha9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 04:03:02,342 - wandb.wandb_agent - INFO - Running runs: ['v3bw7ha9']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 5.8 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 3.4 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 65.7 K\n",
            "33  | model.embedding                                            | Embedding               | 10.6 K\n",
            "34  | model.fc                                                   | Linear                  | 10.7 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.4 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.4 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 396 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 66.0 K\n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 132 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 131 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 256   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 256   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 256   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 396 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 66.0 K\n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 132 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 131 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 256   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 256   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 256   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 396 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 66.0 K\n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 132 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 131 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 256   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 256   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 256   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 396 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 66.0 K\n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 132 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 131 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 256   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 256   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 256   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | model.transformer_decoder.layers.4                         | TransformerDecoderLayer | 396 K \n",
            "96  | model.transformer_decoder.layers.4.self_attn               | MultiheadAttention      | 66.0 K\n",
            "97  | model.transformer_decoder.layers.4.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "98  | model.transformer_decoder.layers.4.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "99  | model.transformer_decoder.layers.4.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "100 | model.transformer_decoder.layers.4.linear1                 | Linear                  | 132 K \n",
            "101 | model.transformer_decoder.layers.4.dropout                 | Dropout                 | 0     \n",
            "102 | model.transformer_decoder.layers.4.linear2                 | Linear                  | 131 K \n",
            "103 | model.transformer_decoder.layers.4.norm1                   | LayerNorm               | 256   \n",
            "104 | model.transformer_decoder.layers.4.norm2                   | LayerNorm               | 256   \n",
            "105 | model.transformer_decoder.layers.4.norm3                   | LayerNorm               | 256   \n",
            "106 | model.transformer_decoder.layers.4.dropout1                | Dropout                 | 0     \n",
            "107 | model.transformer_decoder.layers.4.dropout2                | Dropout                 | 0     \n",
            "108 | model.transformer_decoder.layers.4.dropout3                | Dropout                 | 0     \n",
            "109 | model.transformer_decoder.layers.5                         | TransformerDecoderLayer | 396 K \n",
            "110 | model.transformer_decoder.layers.5.self_attn               | MultiheadAttention      | 66.0 K\n",
            "111 | model.transformer_decoder.layers.5.self_attn.out_proj      | _LinearWithBias         | 16.5 K\n",
            "112 | model.transformer_decoder.layers.5.multihead_attn          | MultiheadAttention      | 66.0 K\n",
            "113 | model.transformer_decoder.layers.5.multihead_attn.out_proj | _LinearWithBias         | 16.5 K\n",
            "114 | model.transformer_decoder.layers.5.linear1                 | Linear                  | 132 K \n",
            "115 | model.transformer_decoder.layers.5.dropout                 | Dropout                 | 0     \n",
            "116 | model.transformer_decoder.layers.5.linear2                 | Linear                  | 131 K \n",
            "117 | model.transformer_decoder.layers.5.norm1                   | LayerNorm               | 256   \n",
            "118 | model.transformer_decoder.layers.5.norm2                   | LayerNorm               | 256   \n",
            "119 | model.transformer_decoder.layers.5.norm3                   | LayerNorm               | 256   \n",
            "Validation sanity check: 0it [00:00, ?it/s]          | 0     \n",
            "121 | model.transformer_decoder.layers.5.dropout2                | Dropout                 | 0     \n",
            "122 | model.transformer_decoder.layers.5.dropout3                | Dropout                 | 0     \n",
            "123 | train_acc                                                  | Accuracy                | 0     \n",
            "124 | val_acc                                                    | Accuracy                | 0     \n",
            "125 | test_acc                                                   | Accuracy                | 0     \n",
            "126 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "127 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "128 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "5.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.8 M     Total params\n",
            "23.121    Total estimated model params size (MB)\n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=3.14, v_num=7ha9, val_loss=4.720, val_cer=0.999]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 95/95 [02:13<00:00,  1.40s/it, loss=3.09, v_num=7ha9, val_loss=2.890, val_cer=0.929]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.65, v_num=7ha9, val_loss=2.890, val_cer=0.929]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.61, v_num=7ha9, val_loss=2.540, val_cer=0.811]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.54, v_num=7ha9, val_loss=2.540, val_cer=0.811]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.52, v_num=7ha9, val_loss=2.460, val_cer=0.821]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.48, v_num=7ha9, val_loss=2.460, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.47, v_num=7ha9, val_loss=2.390, val_cer=0.821]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=2.42, v_num=7ha9, val_loss=2.390, val_cer=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=2.39, v_num=7ha9, val_loss=2.290, val_cer=0.771]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=2.34, v_num=7ha9, val_loss=2.290, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=2.33, v_num=7ha9, val_loss=2.220, val_cer=0.768]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=2.3, v_num=7ha9, val_loss=2.220, val_cer=0.768]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=2.29, v_num=7ha9, val_loss=2.190, val_cer=0.771]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=2.25, v_num=7ha9, val_loss=2.190, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=2.26, v_num=7ha9, val_loss=2.150, val_cer=0.772]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.22, v_num=7ha9, val_loss=2.150, val_cer=0.772]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.22, v_num=7ha9, val_loss=2.120, val_cer=0.770]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.2, v_num=7ha9, val_loss=2.120, val_cer=0.770]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.2, v_num=7ha9, val_loss=2.100, val_cer=0.771]\n",
            "Epoch 10:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.19, v_num=7ha9, val_loss=2.100, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.17, v_num=7ha9, val_loss=2.070, val_cer=0.775]\n",
            "Epoch 11:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.14, v_num=7ha9, val_loss=2.070, val_cer=0.775]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 95/95 [02:13<00:00,  1.41s/it, loss=2.15, v_num=7ha9, val_loss=2.050, val_cer=0.773]\n",
            "Epoch 12:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=2.13, v_num=7ha9, val_loss=2.050, val_cer=0.773]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.13, v_num=7ha9, val_loss=2.020, val_cer=0.779]\n",
            "Epoch 13:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.11, v_num=7ha9, val_loss=2.020, val_cer=0.779]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.12, v_num=7ha9, val_loss=2.000, val_cer=0.781]\n",
            "Epoch 14:  84%|████████▍ | 80/95 [01:09<00:13,  1.14it/s, loss=2.09, v_num=7ha9, val_loss=2.000, val_cer=0.781]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.08, v_num=7ha9, val_loss=1.980, val_cer=0.781]\n",
            "Epoch 15:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.07, v_num=7ha9, val_loss=1.980, val_cer=0.781]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.06, v_num=7ha9, val_loss=1.950, val_cer=0.786]\n",
            "Epoch 16:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.05, v_num=7ha9, val_loss=1.950, val_cer=0.786]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.03, v_num=7ha9, val_loss=1.930, val_cer=0.779]\n",
            "Epoch 17:  84%|████████▍ | 80/95 [01:09<00:13,  1.15it/s, loss=2.02, v_num=7ha9, val_loss=1.930, val_cer=0.779]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2.02, v_num=7ha9, val_loss=1.910, val_cer=0.778]\n",
            "Epoch 18:  84%|████████▍ | 80/95 [01:09<00:13,  1.14it/s, loss=2.02, v_num=7ha9, val_loss=1.910, val_cer=0.778]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=2, v_num=7ha9, val_loss=1.880, val_cer=0.775]   \n",
            "Epoch 19:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.98, v_num=7ha9, val_loss=1.880, val_cer=0.775]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=1.99, v_num=7ha9, val_loss=1.880, val_cer=0.771]\n",
            "Epoch 20:  84%|████████▍ | 80/95 [01:09<00:13,  1.14it/s, loss=1.96, v_num=7ha9, val_loss=1.880, val_cer=0.771]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.95, v_num=7ha9, val_loss=1.840, val_cer=0.776]\n",
            "Epoch 21:  84%|████████▍ | 80/95 [01:09<00:13,  1.14it/s, loss=1.94, v_num=7ha9, val_loss=1.840, val_cer=0.776]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21: 100%|██████████| 95/95 [02:14<00:00,  1.41s/it, loss=1.94, v_num=7ha9, val_loss=1.840, val_cer=0.767]\n",
            "Epoch 22:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.92, v_num=7ha9, val_loss=1.840, val_cer=0.767]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.93, v_num=7ha9, val_loss=1.790, val_cer=0.766]\n",
            "Epoch 23:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.89, v_num=7ha9, val_loss=1.790, val_cer=0.766]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.89, v_num=7ha9, val_loss=1.780, val_cer=0.752]\n",
            "Epoch 24:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.9, v_num=7ha9, val_loss=1.780, val_cer=0.752]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.88, v_num=7ha9, val_loss=1.750, val_cer=0.741]\n",
            "Epoch 25:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.86, v_num=7ha9, val_loss=1.750, val_cer=0.741]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.85, v_num=7ha9, val_loss=1.720, val_cer=0.736]\n",
            "Epoch 26:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.83, v_num=7ha9, val_loss=1.720, val_cer=0.736]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.83, v_num=7ha9, val_loss=1.690, val_cer=0.729]\n",
            "Epoch 27:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.82, v_num=7ha9, val_loss=1.690, val_cer=0.729]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.8, v_num=7ha9, val_loss=1.680, val_cer=0.717] \n",
            "Epoch 28:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.78, v_num=7ha9, val_loss=1.680, val_cer=0.717]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=1.78, v_num=7ha9, val_loss=1.610, val_cer=0.698]\n",
            "Epoch 29:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.75, v_num=7ha9, val_loss=1.610, val_cer=0.698]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.72, v_num=7ha9, val_loss=1.570, val_cer=0.686]\n",
            "Epoch 30:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.7, v_num=7ha9, val_loss=1.570, val_cer=0.686]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.67, v_num=7ha9, val_loss=1.510, val_cer=0.666]\n",
            "Epoch 31:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.64, v_num=7ha9, val_loss=1.510, val_cer=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=1.63, v_num=7ha9, val_loss=1.440, val_cer=0.655]\n",
            "Epoch 32:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.58, v_num=7ha9, val_loss=1.440, val_cer=0.655]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.56, v_num=7ha9, val_loss=1.350, val_cer=0.618]\n",
            "Epoch 33:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.52, v_num=7ha9, val_loss=1.350, val_cer=0.618]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.51, v_num=7ha9, val_loss=1.300, val_cer=0.597]\n",
            "Epoch 34:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.46, v_num=7ha9, val_loss=1.300, val_cer=0.597]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.43, v_num=7ha9, val_loss=1.220, val_cer=0.567]\n",
            "Epoch 35:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.4, v_num=7ha9, val_loss=1.220, val_cer=0.567]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.39, v_num=7ha9, val_loss=1.180, val_cer=0.534]\n",
            "Epoch 36:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.34, v_num=7ha9, val_loss=1.180, val_cer=0.534]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.34, v_num=7ha9, val_loss=1.120, val_cer=0.515]\n",
            "Epoch 37:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.28, v_num=7ha9, val_loss=1.120, val_cer=0.515]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.27, v_num=7ha9, val_loss=0.999, val_cer=0.476]\n",
            "Epoch 38:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.22, v_num=7ha9, val_loss=0.999, val_cer=0.476]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.22, v_num=7ha9, val_loss=0.956, val_cer=0.454]\n",
            "Epoch 39:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.19, v_num=7ha9, val_loss=0.956, val_cer=0.454]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.17, v_num=7ha9, val_loss=0.912, val_cer=0.431]\n",
            "Epoch 40:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.14, v_num=7ha9, val_loss=0.912, val_cer=0.431]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=1.15, v_num=7ha9, val_loss=0.855, val_cer=0.418]\n",
            "Epoch 41:  84%|████████▍ | 80/95 [01:10<00:13,  1.14it/s, loss=1.08, v_num=7ha9, val_loss=0.855, val_cer=0.418]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41: 100%|██████████| 95/95 [02:15<00:00,  1.42s/it, loss=1.07, v_num=7ha9, val_loss=0.812, val_cer=0.390]\n",
            "Epoch 42:  84%|████████▍ | 80/95 [01:09<00:13,  1.14it/s, loss=1.07, v_num=7ha9, val_loss=0.812, val_cer=0.390]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42: 100%|██████████| 95/95 [02:14<00:00,  1.42s/it, loss=1.05, v_num=7ha9, val_loss=0.811, val_cer=0.388]\n",
            "Epoch 43:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=1.02, v_num=7ha9, val_loss=0.811, val_cer=0.388]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=1.01, v_num=7ha9, val_loss=0.733, val_cer=0.352]\n",
            "Epoch 44:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=0.999, v_num=7ha9, val_loss=0.733, val_cer=0.352]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44: 100%|██████████| 95/95 [02:16<00:00,  1.43s/it, loss=0.983, v_num=7ha9, val_loss=0.733, val_cer=0.358]\n",
            "Epoch 45:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=0.96, v_num=7ha9, val_loss=0.733, val_cer=0.358]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45: 100%|██████████| 95/95 [02:16<00:00,  1.44s/it, loss=0.933, v_num=7ha9, val_loss=0.689, val_cer=0.337]\n",
            "Epoch 46:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=0.919, v_num=7ha9, val_loss=0.689, val_cer=0.337]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46: 100%|██████████| 95/95 [02:16<00:00,  1.44s/it, loss=0.897, v_num=7ha9, val_loss=0.655, val_cer=0.311]\n",
            "Epoch 47:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=0.888, v_num=7ha9, val_loss=0.655, val_cer=0.311]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47: 100%|██████████| 95/95 [02:16<00:00,  1.44s/it, loss=0.906, v_num=7ha9, val_loss=0.666, val_cer=0.304]\n",
            "Epoch 48:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=0.853, v_num=7ha9, val_loss=0.666, val_cer=0.304]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48: 100%|██████████| 95/95 [02:16<00:00,  1.44s/it, loss=0.861, v_num=7ha9, val_loss=0.597, val_cer=0.291]\n",
            "Epoch 49:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=0.846, v_num=7ha9, val_loss=0.597, val_cer=0.291]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=0.836, v_num=7ha9, val_loss=0.602, val_cer=0.282]\n",
            "Epoch 49: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=0.836, v_num=7ha9, val_loss=0.602, val_cer=0.282]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:53<00:00,  3.33s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.24399620294570923}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 27509\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_040258-v3bw7ha9/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 6828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1615355806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 0.74084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                     trainer/global_step 3950\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 0.60189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.28191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer █▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 819 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msnowy-sweep-4\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/v3bw7ha9\u001b[0m\n",
            "2021-03-10 05:56:53,062 - wandb.wandb_agent - INFO - Cleaning up finished run: v3bw7ha9\n",
            "2021-03-10 05:56:53,685 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 05:56:53,685 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 32\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.01\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 256\n",
            "\ttf_fc_dim: 1024\n",
            "\ttf_layers: 4\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 16\n",
            "2021-03-10 05:56:53,687 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=32 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.01 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=256 --tf_fc_dim=1024 --tf_layers=4 --tf_nhead=4 --window_stride=8 --window_width=16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 05:56:56.218414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-sweep-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/5iljkf8u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "2021-03-10 05:56:58,697 - wandb.wandb_agent - INFO - Running runs: ['5iljkf8u']\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 7.7 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 3.5 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 2.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 262 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 4.2 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 4.2 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 1.1 M \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 263 K \n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 262 K \n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 1.1 M \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 263 K \n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 262 K \n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 1.1 M \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 263 K \n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 262 K \n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 1.1 M \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 263 K \n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 262 K \n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "7.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.7 M     Total params\n",
            "30.837    Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=4.940, val_cer=0.999]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  84%|████████▍ | 80/95 [00:59<00:11,  1.33it/s, loss=nan, v_num=kf8u, val_loss=4.940, val_cer=0.999]\n",
            "Epoch 0: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:58<00:10,  1.37it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 1: 100%|██████████| 95/95 [01:39<00:00,  1.05s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 80/95 [00:58<00:10,  1.37it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 2: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 80/95 [00:57<00:10,  1.38it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 3: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 4: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 80/95 [01:05<00:12,  1.21it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 5: 100%|██████████| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [00:45<00:08,  1.75it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  84%|████████▍ | 80/95 [01:04<00:12,  1.24it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 6: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 7: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 8:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  84%|████████▍ | 80/95 [01:02<00:11,  1.27it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 8: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 9:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  84%|████████▍ | 80/95 [01:01<00:11,  1.29it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 9: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 10:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  84%|████████▍ | 80/95 [01:00<00:11,  1.31it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 10: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 11:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 11: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 12:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  84%|████████▍ | 80/95 [00:57<00:10,  1.39it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 12: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  84%|████████▍ | 80/95 [00:56<00:10,  1.40it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 13: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 14:  84%|████████▍ | 80/95 [00:45<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  84%|████████▍ | 80/95 [01:05<00:12,  1.22it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 14: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 15:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  84%|████████▍ | 80/95 [01:05<00:12,  1.23it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 15: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 16:  84%|████████▍ | 80/95 [00:46<00:08,  1.71it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  84%|████████▍ | 80/95 [01:03<00:11,  1.26it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 16: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 17:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  84%|████████▍ | 80/95 [01:01<00:11,  1.29it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 17: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 18:  84%|████████▍ | 80/95 [00:46<00:08,  1.73it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  84%|████████▍ | 80/95 [01:00<00:11,  1.32it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 18: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 19:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  84%|████████▍ | 80/95 [00:59<00:11,  1.35it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 19: 100%|██████████| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 20:  84%|████████▍ | 80/95 [00:46<00:08,  1.72it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  84%|████████▍ | 80/95 [00:58<00:10,  1.38it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 20: 100%|██████████| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21:  84%|████████▍ | 80/95 [00:46<00:08,  1.74it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  84%|████████▍ | 80/95 [00:56<00:10,  1.41it/s, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21: 100%|██████████| 95/95 [01:40<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "Epoch 21: 100%|██████████| 95/95 [01:41<00:00,  1.06s/it, loss=nan, v_num=kf8u, val_loss=3.220, val_cer=0.975]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100%|██████████| 16/16 [00:48<00:00,  3.00s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': 0.9744817018508911}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 36351\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_055655-5iljkf8u/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 _runtime 2291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               _timestamp 1615358106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    _step 426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               train_loss nan\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      trainer/global_step 1738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 val_loss 3.2239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  val_cer 0.97465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 test_cer 0.97448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▇█▅▄▂▇▇▄▄▁▄▇▇▇▅▄▄▂▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_cer ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 371 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mblooming-sweep-5\u001b[0m: \u001b[34mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/5iljkf8u\u001b[0m\n",
            "2021-03-10 06:35:12,608 - wandb.wandb_agent - INFO - Cleaning up finished run: 5iljkf8u\n",
            "2021-03-10 06:35:21,668 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-10 06:35:21,669 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tconv_dim: 64\n",
            "\tdata_class: EMNISTLines2\n",
            "\tfc_dim: 1024\n",
            "\tgpus: -1\n",
            "\tloss: transformer\n",
            "\tlr: 0.01\n",
            "\tmax_epochs: 50\n",
            "\tmodel_class: LineCNNTransformer\n",
            "\tnum_workers: 20\n",
            "\tprecision: 16\n",
            "\ttf_dim: 256\n",
            "\ttf_fc_dim: 256\n",
            "\ttf_layers: 6\n",
            "\ttf_nhead: 4\n",
            "\twindow_stride: 8\n",
            "\twindow_width: 8\n",
            "2021-03-10 06:35:21,671 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --conv_dim=64 --data_class=EMNISTLines2 --fc_dim=1024 --gpus=-1 --loss=transformer --lr=0.01 --max_epochs=50 --model_class=LineCNNTransformer --num_workers=20 --precision=16 --tf_dim=256 --tf_fc_dim=256 --tf_layers=6 --tf_nhead=4 --window_stride=8 --window_width=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucas-spangher\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-03-10 06:35:24.057980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msparkling-sweep-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/sweeps/gt2t31i7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lucas-spangher/fsdl_lab_5/runs/pzk81bv9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210310_063523-pzk81bv9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "wandb: WARNING Config item 'gpus' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'precision' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'data_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'conv_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_width' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_fc_dim' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'tf_nhead' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).\n",
            "wandb: WARNING Config item 'loss' was locked by 'sweep' (ignored update).\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "EMNISTLines2 loading data from HDF5...2021-03-10 06:35:26,681 - wandb.wandb_agent - INFO - Running runs: ['pzk81bv9']\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1315: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1329: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
            "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 8.4 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 4.4 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 3.1 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 640   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 640   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 36.9 K\n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 36.9 K\n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 36.9 K\n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 36.9 K\n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 36.9 K\n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 36.9 K\n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 73.9 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 73.9 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 147 K \n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 147 K \n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 295 K \n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 295 K \n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 590 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 590 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 1.8 M \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 1.8 M \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 1.0 M \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 262 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 4.0 M \n",
            "Validation sanity check: 0it [00:00, ?it/s]  | 4.0 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]      | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | model.transformer_decoder.layers.4                         | TransformerDecoderLayer | 659 K \n",
            "96  | model.transformer_decoder.layers.4.self_attn               | MultiheadAttention      | 263 K \n",
            "97  | model.transformer_decoder.layers.4.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "98  | model.transformer_decoder.layers.4.multihead_attn          | MultiheadAttention      | 263 K \n",
            "99  | model.transformer_decoder.layers.4.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "100 | model.transformer_decoder.layers.4.linear1                 | Linear                  | 65.8 K\n",
            "101 | model.transformer_decoder.layers.4.dropout                 | Dropout                 | 0     \n",
            "102 | model.transformer_decoder.layers.4.linear2                 | Linear                  | 65.8 K\n",
            "103 | model.transformer_decoder.layers.4.norm1                   | LayerNorm               | 512   \n",
            "104 | model.transformer_decoder.layers.4.norm2                   | LayerNorm               | 512   \n",
            "105 | model.transformer_decoder.layers.4.norm3                   | LayerNorm               | 512   \n",
            "106 | model.transformer_decoder.layers.4.dropout1                | Dropout                 | 0     \n",
            "107 | model.transformer_decoder.layers.4.dropout2                | Dropout                 | 0     \n",
            "108 | model.transformer_decoder.layers.4.dropout3                | Dropout                 | 0     \n",
            "109 | model.transformer_decoder.layers.5                         | TransformerDecoderLayer | 659 K \n",
            "110 | model.transformer_decoder.layers.5.self_attn               | MultiheadAttention      | 263 K \n",
            "111 | model.transformer_decoder.layers.5.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "112 | model.transformer_decoder.layers.5.multihead_attn          | MultiheadAttention      | 263 K \n",
            "113 | model.transformer_decoder.layers.5.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "114 | model.transformer_decoder.layers.5.linear1                 | Linear                  | 65.8 K\n",
            "115 | model.transformer_decoder.layers.5.dropout                 | Dropout                 | 0     \n",
            "116 | model.transformer_decoder.layers.5.linear2                 | Linear                  | 65.8 K\n",
            "117 | model.transformer_decoder.layers.5.norm1                   | LayerNorm               | 512   \n",
            "118 | model.transformer_decoder.layers.5.norm2                   | LayerNorm               | 512   \n",
            "119 | model.transformer_decoder.layers.5.norm3                   | LayerNorm               | 512   \n",
            "120 | model.transformer_decoder.layers.5.dropout1                | Dropout                 | 0     \n",
            "121 | model.transformer_decoder.layers.5.dropout2                | Dropout                 | 0     \n",
            "122 | model.transformer_decoder.layers.5.dropout3                | Dropout                 | 0     \n",
            "123 | train_acc                                                  | Accuracy                | 0     \n",
            "124 | val_acc                                                    | Accuracy                | 0     \n",
            "125 | test_acc                                                   | Accuracy                | 0     \n",
            "126 | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "127 | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "128 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "8.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "8.4 M     Total params\n",
            "33.462    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Epoch 0:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=nan, v_num=1bv9, val_loss=5.120, val_cer=0.998]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 2:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 3:  84%|████████▍ | 80/95 [01:11<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 95/95 [02:25<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 4:  84%|████████▍ | 80/95 [01:11<00:13,  1.12it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 95/95 [02:25<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 5:  84%|████████▍ | 80/95 [01:11<00:13,  1.11it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 95/95 [02:26<00:00,  1.54s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 6:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 95/95 [02:24<00:00,  1.53s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 7:  84%|████████▍ | 80/95 [01:10<00:13,  1.13it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 95/95 [02:24<00:00,  1.52s/it, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]\n",
            "Epoch 8:   0%|          | 0/95 [00:00<?, ?it/s, loss=nan, v_num=1bv9, val_loss=3.770, val_cer=0.986]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q7smXzQinE3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}